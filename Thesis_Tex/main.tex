\documentclass[12pt,oneside]{article}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%   Zusaetzliche Pakete  %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\usepackage{enumerate}  
\usepackage{fancyhdr}
\usepackage{a4wide}
\usepackage{graphicx}
\usepackage{palatino}
\usepackage{multirow}
\usepackage{booktabs}
\usepackage{titlesec}
\usepackage{float}
\usepackage[table]{xcolor}
\usepackage{amssymb}
\usepackage{xcolor}
\usepackage[utf8]{inputenc}

\usepackage{enumitem}% http://ctan.org/pkg/enumitem

%folgende Zeile auskommentieren für englische Arbeiten
%\usepackage[ngerman]{babel}
%folgende Zeile auskommentieren für deutsche Arbeiten
\usepackage[english]{babel}

\usepackage[T1]{fontenc}
\usepackage{paratype}\renewcommand{\familydefault}{\sfdefault}
\usepackage[utf8]{inputenc}
\usepackage[bookmarks]{hyperref}
\usepackage[justification=centering]{caption}
\usepackage[style=authoryear,natbib=true,backend=biber,maxbibnames=20]{biblatex}
\usepackage{csquotes}
\usepackage{booktabs}
\usepackage{makecell}
\bibliography{literatur}

\setlength{\parindent}{0em} 
\setlist[itemize]{noitemsep, topsep=0pt}
\setlist[enumerate]{noitemsep, topsep=0pt}

\colorlet{myLightGreen}{green!10!gray} 
\colorlet{myLightBlue}{blue!20}   

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Definition der Kopfzeile %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\pagestyle{fancy}
\fancyhf{}
\cfoot{\thepage}
\setlength{\headheight}{16pt}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%  Definition des Deckblattes und der Titelseite  %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\newcommand{\JMUTitle}[9]{

  \thispagestyle{empty}
  \vspace*{\stretch{1}}
  {\parindent0cm
  \rule{\linewidth}{.7ex}
  }
  \begin{flushright}
    \sffamily\bfseries\Huge
    #1\\
    \vspace*{\stretch{1}}
    \sffamily\bfseries\large
    #2\\
    \vspace*{\stretch{1}}
    \sffamily\bfseries\small
    #3
  \end{flushright}
  \rule{\linewidth}{.7ex}

  \vspace*{\stretch{1}}
  \begin{center}
    \includegraphics[width=2in]{siegel} \\
    \vspace*{\stretch{1}}
    \Large #5 \\

    \vspace*{\stretch{2}}
   \large Lehrstuhl f\"{u}r Wirtschaftsinformatik und Business Analytics\\
    \large Universität Würzburg\\
    \vspace*{\stretch{1}}
    \large Betreuer:  #8 \\[1mm]
    \large Assistent:  #9 \\[1mm]
    \vspace*{\stretch{1}}
    \large #6, den #7
  \end{center}
}

\titlespacing*{\section}
{0pt}{3.5ex plus 1ex minus .2ex}{.2ex plus .2ex}
\titlespacing*{\subsection}
{0pt}{1.5ex plus 1ex minus .2ex}{.2ex plus .2ex}
\titlespacing*{\subsubsection}
{0pt}{1.5ex plus 1ex minus .2ex}{.2ex plus .2ex}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%  Beginn des Dokuments  %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}
  \JMUTitle
      {Time Series Anomaly Detection Benchmarking}        % Titel der Arbeit
      {Philip Spaier}                        % Vor- und Nachname des Autors
      {3110375}
      
      {Seminararbeit } % Art der Arbeit
      {Würzburg}                           % Ort der Erstellung
      {05.04.2025}                          % Tag der Abgabe
      {Prof. Dr. Gunther Gust}           % Name des Erstgutachters
      {Viet Nguyen} % Name des/der betreuenden Assistent/Assistentin
      
  \clearpage

\lhead{}
\pagenumbering{Roman} 
    \setcounter{page}{1}

\tableofcontents
\clearpage

\addcontentsline{toc}{section}{\listfigurename}
\listoffigures

\addcontentsline{toc}{section}{\listtablename}
\listoftables
\clearpage

\setlength{\parskip}{0.5em} 


%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%  Kurzzusammenfassung   %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\lhead{Abstract}
\section*{Abstract}

Eine Kurzzusammenfassung der Vorgehensweise und der wesentlichen Ergebnisse.

Allgemeine Merkmale
\begin{itemize}
    \item Objektivität: Es soll sich jeder persönlichen Wertung enthalten.
    \item Kürze: Es soll so kurz wie möglich sein.
    \item Verständlichkeit: Es weist eine klare, nachvollziehbare Sprache und Struktur auf.
    \item Vollständigkeit: Alle wesentlichen Sachverhalte sollen enthalten sein.
    \item Genauigkeit: Es soll genau die Inhalte und die Meinung der Originalarbeit wiedergeben.
\end{itemize}{}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%  Einstellungen  %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\clearpage
\pagenumbering{arabic}  
    \setcounter{page}{1}
\lhead{\nouppercase{\leftmark}}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%  Hauptteil  %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\section{Literature Review} \label{einleitung}
Time Series Anomaly Detection (TSAD), as a subcategory of the broader field of Anomaly Detection, has seen increased attention since the start of the twenty first century. With the internet having established itself as a persistent and omnipresent force in every imaginable aspect of human life, time series data can be found in abundance. Modern developments in Internet-of-Things (IoT) applications, the digitization of financial data, and a massive rise in the consumption of streaming services have contributed to an exponential growth of time series data [source needed]. This in turn has made the manual search of potential anomalies in many fields completely infeasible, leading to an increased demand for automated anomaly detection methods. While there is a continuously growing repertoire of such automated detection methods, the lack of a generally accepted and reliable benchmark makes not just further developments but also the selection of appropriate models difficult. In the following sections of this literature review, I will provide the reader with a better understanding of context independent Time Series Anomaly Detection, the most commonly applied methods, and the current state of benchmarking.


\subsection{Time Series Data and Anomaly Detection Definition}
Time Series Data, as used in the rest of this thesis, shall be defined as follows: a sequence of data or observations, typically indexed by or associated with specific timestamps, collected in chronological order over a period of time.
For the purpose of analysis, continuous signals must be converted into individual data points. Each datapoint can either represent a binary state (1 or 0), be a numerical value measured on a ratio scale (eg. number of occurrences), or a numerical value measured on an interval scale (eg. temperature on a Celsius scale).
A time series with a dimensionality of one (only a single feature) will be referred to as "univariate", while a time series with higher dimensionality (multiple features) will be referred to as "multivariate". \\
An anomaly will be defined as follows: an abnormal, rarely occurring data point or sequence, that has to be be detectable with exclusively context independent methods. Individual anomalous data points will be referred to as "point based" anomalies. Multiple consecutive anomalous points, each of which might be unremarkable on their own, while displaying unusual behavior as a sequence,  will be referred to as "sequence based" or "collective" anomalies \parencites[p.~3]{liu2024elephant}[p.~8]{chalapathy2019deeplearninganomalydetection}. A separate category of anomalies would be context dependent ones. Those are data points or sequences, possibly indistinguishable from normal ones if analyzed without context, but if combined with additional information about the field or time series, are considered anomalous \citep[pp.~7-8]{chalapathy2019deeplearninganomalydetection}. Context dependent anomalies will not be topic of the research presented here. \\
Given those definitions, Time Series Anomaly Detection is therefore the task of correctly and autonomously identifying anomalies within a given time series.

Insert Images of point vs sequence.

\subsection{Relevant Fields}

The following is an overview of fields relying on Time Series Anomaly Detection. It is a non-exhaustive list, simply highlighting some of the most prominent use cases to provide context. \

\textbf{Illicit Activity and Fraud Detection:} With the global financial system relying primarily on digital transactions, it has become crucial to detect fraudulent activities as quickly and accurately as possible. A particularly obvious example is credit card fraud, creating an estimated yearly loss in the billions of dollar \parencites[p.~2]{FinFraud}. Companies like Visa and Mastercard put great emphasis on being able to detect anomalous transactions in real time to then analyses them and prevent potential harm to their customers \citep{VisaFr}. While credit card fraud is a prominent application, the scope of financial anomaly detection extends significantly further, playing a critical role in the operations of stock exchanges, brokerage firms, and banks. These institutions leverage anomaly detection techniques to identify various illicit activities, ensure market integrity, manage operational risks, and comply with stringent regulatory requirements \citep{DBSurv}. \

\textbf{Healthcare:} Healthcare critically relies on analyzing physiological signals, such as those captured by the electrocardiogram (ECG), which provides vital time series data reflecting the heart's electrical activity. While historically, ECG analysis has focused on identifying established patterns of known heart diseases, this approach often fails to detect rare or atypical anomalies that do not fit predefined categories, potentially missing critical conditions. To address this issue, Time Series Anomaly Detection has been introduced for the purpose of detecting such rare anomalies that would go unnoticed by conventional pattern classification \parencites[p.~1-2]{jiang2024anomalydetectionelectrocardiogramsadvancing}.

\textbf{Website Traffic:} A common threat faced by web-services are so called Denial or Service (DoS) and Distributed Denial of Service (DDoS) attacks. These include hitting a webserver with so many requests that the systems becomes inoperational and can no longer service legitimate users \citep{BSI1}. A significant challenge in detecting these attacks is that the malicious traffic can often mimic normal network traffic, making it difficult for traditional packet-based intrusion detection systems or statistical methods reliant on fixed thresholds to accurately identify attacks, especially when they are hidden within legitimate flows. Time series analysis allows systems to observe and distinguish the instant changes in network traffic that indicate an attack, even when individual packets or simple statistics are insufficient. Time series anomaly detection provides a means to autonomously identify and localize potentially harmful deviations within the network traffic and thereby ensure the availability and reliability of services \parencites[pp.~1-2]{FOULADI2020102587}. \par
The list extends far beyond the fields named above. Time Series Anomaly Detection can be also found in astronomy \citep{astro}, earth sciences, manufacturing \parencites[p.~1]{Zamanzadeh_Darban_2024}, cybersecurity, and law enforcement \parencites[p.~1]{boniol2024divetimeseriesanomalydetection}.


\subsection{Detection Methods}
Detection methods, in common descriptions and within the scientific literature, are often grouped or distinguished by a variety of aspects. This categorization can sometimes lack a consistent taxonomy. To provide a clearer framework, I will now systematically explain and categorize these methods through three key perspectives:
\begin{itemize}
    \item Degree of supervision
    \item Architecture
    \item Technique
\end{itemize}

\subsubsection{Degree of Supervision} \textit{Unsupervised models} operate on data without any explicit labels distinguishing normal from anomalous instances. While they don't require pre-labeled data, they typically do require a training or fitting phase. During this phase, the model learns the inherent structure, patterns, distributions, or densities from the unlabeled dataset.\par \textit{Semi-supervised models} are trained exclusively on data that is known or assumed to be 'normal.' They do not require labeled anomalies for training. The model learns a precise representation or boundary of this normal behavior. During deployment, any new data instance that significantly deviates from this learned model of normalcy is flagged as an anomaly. \par \textit{Supervised models} require a dataset where both normal and anomalous instances are explicitly labeled beforehand. The model is then trained to learn the distinguishing features or decision boundaries that separate these classes, effectively treating anomaly detection as a (often highly imbalanced) classification problem. \parencites[pp.~5-6]{boniol2024divetimeseriesanomalydetection}[p.~3]{liu2024elephant}[p.~3-4]{SchmidlEtAl2022Anomaly}.

\subsubsection{Architecture} \textit{Statistical models} identify anomalies by relying on statistical assumptions to detect deviations from expected data distributions. They often involve fitting a distribution model to the data and measuring abnormality based on probabilities or distances from the calculated distribution. Statistical models often require a threshold to be set beforehand\parencites[p.~6-7]{liu2024elephant}[p.~1]{FOULADI2020102587}. \par \textit{Neural Network based models} are a collection of distributed, adaptive, non-linear processing units with adjustable weights \parencites[p.~427]{GURESEN2011426}. They rely on a training dataset and are often semi-supervised. Deep neural networks, a subcategory of neural networks, model spacial and temporal dependencies \parencites[p.~6-7]{liu2024elephant}[p.~6]{Zamanzadeh_Darban_2024}.\par \textit{Foundational Models} utilize transfer learning, using knowledge from a different class of tasks and then applying it on the target task. These models are pre-trained and are then being fine-tuned \parencites[p.~4]{bommasani2022opportunitiesrisksfoundationmodels}. In the context of TSAD, those models are GPT models fine tuned on time series data, general purpose time series models, or originally time series classification models now used for anomaly detection \parencites[p.~7]{liu2024elephant}.

\subsubsection{Technique} \textit{Distance based models} work on the idea that anomalous points or sequences will further away when using a distance measurement. They can be either be compared to their nearest neighbor, all other points/subsequences, or cluster centers \parencites[p.~6]{SchmidlEtAl2022Anomaly}. Such distances are calculated in various ways depending on the model and implementation, with the most common definitions being the Euclidean distance or the Z-normalized Euclidean distance. Distance based models use only the x- and y-axis data, with no labels being required \parencites[p.~8]{boniol2024divetimeseriesanomalydetection}. \par \textit{Forecasting models} learn the normal patters of a time series and, often using a sliding context window, forecast the next datapoint in the series. The forecasted and actual data points are then compared, with the difference being used for an anomaly score. Given a high enough anomaly score, a point is considered an anomaly. Such models are usually semi-supervised \parencites[p.~4-5]{SchmidlEtAl2022Anomaly}. \par \textit{Isolation Tree Models} use ensembles of random trees, selecting random features and splits, to separate points or sequences from each other. It operates on the idea that anomalies require fewer steps to be separated from the rest of the data than normal points/sequences. For each point/sequence, the distance from the root is calculated. The shorter a distance is, the more likely is a point/sequence to be an anomaly. These models can be both unsupervised and supervised \parencites[p.~6-7]{SchmidlEtAl2022Anomaly} \par \textit{Distribution based models} estimate a distribution of the time series and then score individual points or sequences as anomalous or normal based on it. Anomalous points are expected to have a low probability. Alternatively to probabilities, the anomaly score can also be calculated using likelihoods or distances. These models are generally unsupervised or occasionally semi-supervised \parencites[p.~6]{SchmidlEtAl2022Anomaly}. \par \textit{Graph based models} methods turn time series data, or parts of it, into a graph structure. This graph represents the different types of patterns (subsequences) found in the data as nodes, and how these patterns follow each other over time as connections (edges) between the nodes. Anomalies are then determined based on usual structures or behaviors found in the graph \parencites[p.~23-24]{boniol2024divetimeseriesanomalydetection}. Graph based time series models can be further divided in multiple subcategories, including AutoEncoder- and GAN-based methods, as well as predictive graph models \parencite{ho2025graphanomalydetectiontime}. \par \textit{Reconstruction models} learn a time series' features and patterns by encoding normal data into a low dimensional space. Given a test dataset, they compress test data and reconstruct it using their model based on that low-dimensional space. Should a point or sequence of this reconstructed version deviate substantially from the actual data, then it is labeled as anomalous. These models are often considered semi-supervised because they typically use normal labeled data for training. However, models that do not rely on a training dataset and instead directly encode and reconstruct the test data also exist, operating in an unsupervised manner. \parencites[p.~5]{SchmidlEtAl2022Anomaly}. \par \textit{Encoder based models} operate similarly to reconstruction models. They compress a given time series into a low-dimensional representation, but instead of reconstructing it, they directly compare this compressed version to their model of normal time series. Anomalous points or sequences might have unusual encoded representations, and their deviations from the normal model are then used to calculate an anomaly score \parencites[p.~5-6]{SchmidlEtAl2022Anomaly}.

\subsection{Performance Metrics}

For the effective evaluation of a models performance, as defined by \cite{papaVus}, metrics have to fullfil the following criteria:
\begin{itemize}
    \item \textit{Robustness to Lag:} The evaluation measure should be insensitive to slight temporal shifts or lags in anomaly scores.
    \item \textit{Robustness to Noise:} The evaluation measure should be stable and unaffected by noise in the anomaly scores.
    \item \textit{Robustness to Anomaly Cardinality Ratio:} The evaluation measure's score should not be influenced by the proportion of anomalies in the data.
    \item \textit{High Separability between Accurate and Inaccurate Methods:} The measure must effectively distinguish between accurate and inaccurate detection methods.
    \item \textit{Consistency:} The measure should produce repeatable scores for similar data and consistently rank different methods.
\end{itemize}
Commonly applied performance measures for TSAD can generally be classified based on two characteristics: Point-wise or Range-wise, and Threshold-dependent or Threshold-independent.


\subsubsection{Point-wise or Range-wise}
\textit{Point-wise evaluation measures} look at each anomalous point independently, determining in a binary fashion whether a model classified them correctly as normal or anomalous \parencite[p.~7]{liu2024elephant}. These measures suffer from a variety of issues. Most crucially, they can unfairly penalize methods that detect only part of an anomalous range or whose detection peak doesn't perfectly align with the labeled range. Further more, they are sensitive to temporal lag. Should an anomalous data point be detected slightly before or after the actual anomaly occurs, a fully point-wise metric will score it with an unreasonably low score \parencites[p.~2778]{papaVus}.\\ \textit{Range-wise measures} look at anomalies not just from the perspective of individual points but take sequences into consideration. For anomalous sequences, their evaluation can involve determining how much the detected and the actual sequence overlap. Additionally, such measures may incorporate strategies like adequately handling lag (e.g., by considering an anomaly detected if it's within a specified range of an actual one, even if not at the exact spot) or including a cardinality factor to penalize models that incorrectly segment anomalies (such as detecting multiple short ones for a single large event, or vice-versa) \parencite[p.~7]{liu2024elephant}.
\subsubsection{Threshold-dependent or Threshold-independent}
\textit{Threshold-dependent measures} require a threshold to be set that determines whether an anomaly score classifies a value as anomalous or normal. This can be done based on statistical assumptions, or using dynamic algorithms that adjust to the data and results \parencites[p.~38-39]{boniol2024divetimeseriesanomalydetection}. Setting these thresholds automatically, however, is often difficult when working with large and diverse datasets, and the chosen thresholds can drastically change a metric's accuracy. Noise and the normal-to-anomalous ratio in a time series can be particularly problematic \parencites[p.~2777-2778]{papaVus}. \\
\textit{Threshold-independent measures} evaluate the performance of a time series anomaly detection method without needing a specific score cutoff to decide what constitutes an anomaly. Instead of relying on a fixed threshold, they assess how effectively the method's anomaly scores rank true anomalies higher than normal data points across the entire range of scores \parencites[p.~39-41]{boniol2024divetimeseriesanomalydetection}.

\subsubsection{Definition and Classifgication of Metrics}

The following defines the most commonly used metrics and classifies them into the above described categories \parencites[p.2776-2780]{papaVus}:
\begin{itemize}
    \item \textit{Precision / Range Precision:} number of correctly identified anomalies over all anomalies.
    \item \textit{Recall (TPR) / Range Recall:} number of correctly identified anomalies over all anomalies.
    \item \textit{F-Score / Range F-Score:} Harmonic Mean of Precision and Recall. 
    \item \textit{False Positive Rate (FPR):} number of points wrongly identified as anomalies over the total number of normal points.
    \item \textit{AUC-ROC:} area under the curve corresponding to TPR on the y-axis and FPR on the x-axis at all threshold levels.
    \item \textit{AUC-Precision:} area under the curve corresponding to the Recall on the x-axis and Precision on the y-axis at all threshold levels.
    \item \textit{VUS-ROC:} generating multiple ROC curves for a range of different buffer lengths. These stacked ROC curves form a 3D surface, and VUS-ROC is the volume beneath this surface.
    \item \textit{VUS-Precision:} generating multiple Precision-Recall curves for a range of different buffer lengths. These stacked PR curves form a 3D surface, and VUS-Precision (VUS-PR) is the volume beneath this surface.
\end{itemize}




\begin{table}[htbp]
  \centering
  \label{tab:evaluation_categories}

  \begin{tabular}{lcc}
    \toprule
    & Threshold-dependent & Threshold-independent \\
    \midrule
    Point-wise & \makecell[l]{Precision \\ Recall \\ False Positive Rate \\ F-Score} & \makecell[l]{AUC-ROC \\ AUC-Precision} \\
    \midrule
    Range-wise & \makecell[l]{Range Precision \\ Range Recall \\ Range F-Score} & \makecell[l]{VUS-ROC \\ VUS-Precision} \\
    \bottomrule
  \end{tabular}
  \caption{Evaluation Measures}

\end{table}

\textbf{}


\subsection{State of Benchmarking}

While time series anomaly detection is a well-established field, most advancements in systematic bench marking have been made within the last decade. The following will provide an overview over the most important papers and datasets contributing to this endeavor. As always, given the large corpus of work, this is not an exhaustive list.  \par
\textbf{Yahoo (2015)}: Yahoo provides one of the earliest available labeled large scale TSAD datasets. It consists of real data with time series from various Yahoo services and synthetic data, containing trends, noise, and seasonality \parencite{Yahoo}. \par
\textbf{Numenta Anomaly Benchmark (NAB), 2015:} The Numeta Anomaly Benchmark is often considered to be the first large scale open source benchmarking environment for TSAD. At the time of release, it contained 58 datasets, made up of a mixture of artificial and real time series. The labels are first created by multiple humans, then combined into a ground truth by an algorithm. NAB uses a custom threshold dependent scoring function for the evaluation of a model's performance, designating high value to an algorithms ability to detect an anomaly as early as possible. It is designed specifically for real time anomaly detection, not static analysis. Therefore, only unsupervised models can be tested, with no training/test split of the data \parencites{Lavin_2015}.\par
\textbf{Illusion of Progress (2021):} \cite{wu2021current} provide substantial criticism regarding the state of TSAD benchmarking. The authors find most previously created datasets to contain one or multiple of the following flaws:
\begin{itemize}
    \item \textit{Triviality:} Many anomalies are "are so simple that solving them seems pointless or even absurd" \parencites[p.~2]{wu2021current}.
    \item \textit{Unrealistic Anomaly Density:} Many time series have anomaly rates so high that they can realistically no longer be defined as anomalous. The task turns into a classification problem.
    \item \textit{Mislabeled Ground Truth:} Many time series have data that, without context, appears to be mislabeled. Normal data points are falsely labeled as anomalies and vice versa.
    \item \textit{Run-to-failure Bias:} In the case of real data, many systems are operated until failure. This results in an unusually high anomaly count towards the end of the dataset.
\end{itemize}
The authors introduce their UCR Time Series Anomaly Archive, a collection of 250 curated univariate time series from human medicine, biology, meteorology and industry to provide a dataset that combats these issues \parencite{wu2021current}. \par
\textbf{Exathlon (2021):} \cite{jacob2021exathlonbenchmarkexplainableanomaly} introduce the first public benchmarking suite for multivariate time series anomaly detection. Their dataset consists of time series collected 100 executions of 10 distributed streaming jobs on a Spark cluster. The datasets contain primarily sequence based anomalies; the tested models are primarily semi-supervised. Exathlon evaluates detection performance (Precision, Recall, F-Score, and AUPRC) and computational efficiency. \par
\textbf{TODS (2021):} \cite{NEURIPS} contribute crucially to the taxonomy for outliers and synthetic anomaly injection. 'Point-wise' outliers. including their subcategories 'global outliers' and 'contextual outliers', as well as 'pattern-wise' outliers, including their subcategories 'shapelet outliers', 'seasonal outliers', and 'trend outliers' are introduced. The authors provide 35 new synthetic datasets and 4 new multivariate real datasets, in addition to 9 existing datasets for their benchmark. \par
\textbf{GutenTAG (2022):} \cite{SchmidlEtAl2022Anomaly} implement and evaluate 71 different algorithms on 976 time series across a variety of fields, both univariate and multivariate. Further more, they introduced the GutenTAG synthetic dataset generator, allowing the creation of time series with with different lengths, variances, amplitudes, frequencies, and dimensions. \par
\textbf{TSD-UAD (2022):} \cite{paparrizos2022tsb} introduce TSB-UAD, a new comprehensive end-to-end benchmark suite designed for evaluating univariate TSAD methods. The benchmark aims to address limitations in current practices, such as the reliance on biased proprietary/synthetic data or limited public datasets. TSB-UAD provides a reproducible platform for researchers by collecting, processing, and formatting a large and diverse set of time series with labeled anomalies.
The TSB-UAD suite encompasses 13,766 univariate time series across various domains, featuring high variability in anomaly types (point, contextual, collective), ratios, and sizes. It includes 18 previously proposed public datasets and contributes two new collections: 126 "artificial" datasets derived from transforming time-series classification data (leveraging the UCR Archive) and 92 "synthetic" datasets generated by applying various global, local, and subsequence transformations to public data to introduce new anomalies and increase detection difficulty. The benchmark suite also provides a Python library to handle pre-processing, post-processing, data generation, transformation, and includes statistical analysis methods (Friedman, Nemenyi tests) for comparing algorithms. It evaluates 12 representative AD methods and introduces measures (Relative Contrast (RC), Normalized Clusteredness of Abnormal Points (NC), Normalized Adjacency of Normal/Abnormal Cluster (NA)) to quantify dataset difficulty. Data and code are made publicly available in a Github repository. \par
\textbf{TSD-AD (2024):} TSB-AD is presented as a new comprehensive benchmark suite for univariate and multivariate time-series anomaly detection (TSAD), designed to address limitations in existing evaluations stemming from flawed datasets, unreliable measures, and inconsistent practices. The benchmark offers 1070 high-quality, curated time series derived from 40 diverse public datasets, substantially increasing the scale and integrity of available data for benchmarking. TSB-AD includes 40 representative AD algorithms spanning statistical, neural network, and foundation model categories. The paper identifies VUS-PR as a robust and reliable evaluation metric, contrasting it with traditional measures prone to biases like Point Adjustment and sensitivity to lag. TSB-AD is released open-source to provide a stable platform for research and establish a leaderboard \parencites{liu2024elephant}.

When looking at the evaluation results of all major benchmarks, it is difficult to point to any specific model as the conclusively best. Given the constantly evolving benchmarking criteria and evolution of dataset quality, this is not surprising. Overall, traditional models have been found across multiple papers and benchmarks to rival, or in many case outperform, newer more complex architectures. Foundational models are promising for point-wise anomalies but get beaten decisively for sequence-wise anomalies. [insert sources]

\section{Dataset Analysis}

\section{Replication of TSB-AD Benchmark Results}

\subsection{Setup and Implementation}
For the recreation of the TSB-AD benchmark results, the following core components were used:
\begin{itemize}
    \item AMD Ryzen 5 7500F, 6C/12T, 3.70-5.00GHz
    \item Nvidia 3060TI 8 GB GDDR6 VRAM
    \item 32 GB DDR5 RAM
    \item OS: Windows 11
\end{itemize}

For the actual implementation, the TSB-AD Github repository was cloned and instructions provided by the authors on how to set up the correct development environment were followed. Given the high number of datasets, the benchmark was performed primarily running 3 models in parallel. Two non-resource intensive models using only the CPU (Statistical Methods as classified by the authors of TSB-AD) and one GPU utilizing model (Neural Network-based Method or Foundation Model-based Method). GPU utilization would frequently reach 100\%, whereas CPU utilization rarely even exceeded 60\%.\par
Despite extensive efforts, I was not able to get all provided anomaly detection models to run reliably. These are the models which I could not successfully implement:

\begin{itemize}
    \item \textbf{MomentFM, TimesFM, and TimesFT} require packages that cause unresolvable dependency incompatibilities with TSB-AD.
    \item \textbf{Chronos and OFA} have required packages that do not work on my system for an unidentified reason. 
    \item \textbf{Lag-Lama} has a required file which I can not find in the provided Github repository.
    \item \textbf{EIF} causes the system to run out of memory and crash.
    \item \textbf{Donut} creates a for me unidentifiable error, seemingly stemming from the donut.py file itself.
    \item \textbf{NORMA} and Series2Graph are commercially licensed models. 
\end{itemize}

\subsection{Results}
\subsubsection{Univariate Results}
Since \cite{liu2024elephant} only publish the full VUS-PR results for each individual dataset and model, the comparison will focus on this metric. Table 2 provides information about the differences found between both benchmark evaluations for univariate detection models.

\begin{table}[htbp]
    \centering
    \label{tab:vus_pr_diff_summary_uni}
    \begin{tabular}{lrrrrrrr}
        \toprule
        \textbf{Model} & \textbf{\makecell[r]{Avg. Abs.\\Diff.}} & \textbf{\makecell[r]{Max. Abs.\\Diff.}} & \textbf{\makecell[r]{File\\ID}} & \textbf{\makecell[r]{Num. of\\Anom.}} & \textbf{\makecell[r]{Datasets\\>5\% Diff.}} & \textbf{\makecell[r]{Datasets\\>25\% Diff.}} & \textbf{\makecell[r]{Datasets\\>50\% Diff.}} \\
        \midrule
        \rowcolor{lightgray} IForest & 0.163 & 0.999 & 552 & 1 & 195 & 81 & 24 \\
        \rowcolor{myLightBlue} LSTMAD & 0.108 & 0.951 & 849 & 2 & 118 & 36 & 12 \\
        \rowcolor{myLightBlue} USAD & 0.106 & 0.982 & 849 & 2 & 111 & 44 & 27 \\
        \rowcolor{myLightBlue} TranAD & 0.093 & 0.934 & 865 & 2 & 109 & 47 & 19 \\
        \rowcolor{myLightBlue} OmniAnomaly & 0.082 & 0.966 & 865 & 2 & 101 & 44 & 13 \\
        \rowcolor{myLightBlue} CNN & 0.079 & 0.987 & 645 & 1 & 125 & 29 & 12 \\
        \rowcolor{myLightBlue} AnomalyTrans & 0.053 & 0.603 & 191 & 2 & 26 & 10 & 5 \\
        \rowcolor{lightgray} KShapeAD & 0.048 & 0.999 & 780 & 1 & 71 & 14 & 8 \\
        \rowcolor{lightgray} SAND & 0.047 & 0.994 & 680 & 1 & 70 & 17 & 7 \\
        \rowcolor{myLightBlue} FITS & 0.038 & 0.450 & 579 & 1 & 86 & 6 & 0 \\
        \rowcolor{myLightGreen} MOMENT\_ZS & 0.007 & 0.413 & 579 & 1 & 9 & 1 & 0 \\
        \rowcolor{myLightBlue} AutoEncoder & 0.006 & 0.152 & 658 & 1 & 8 & 0 & 0 \\
        \rowcolor{lightgray} LOF & 0.002 & 0.075 & 811 & 3 & 2 & 0 & 0 \\
        \rowcolor{lightgray} POLY & 0.001 & 0.288 & 534 & 1 & 2 & 1 & 0 \\
        \rowcolor{lightgray} MatrixProfile & 5.96e-05 & 0.015 & 017 & 1 & 0 & 0 & 0 \\
        \rowcolor{lightgray} SR & 8.24e-06 & 0.002 & 534 & 1 & 0 & 0 & 0 \\
        \bottomrule
    \end{tabular}
    \par
    \vspace{1em}
    \noindent 
    \begin{tabular}{@{} p{1em} l @{}} 
        \rowcolor{lightgray} \strut & Statistical Methods \\
        \rowcolor{myLightBlue} \strut & Neural Network-based Method \\
        \rowcolor{myLightGreen} \strut & Foundation Model-based Method \\
        
    \end{tabular}
    \caption{\label{tab:Table 2} Summary of absolute differences in VUS-PR for univariate datasets.}
\end{table}

For most models, the average difference stayed firmly below 0.100, with IForest, LSTMAD, and USAD being the main exceptions. Beyond these averages, roughly half of all models had at least one dataset where the absolute score differed by more than 0.950. Furthermore, over a third of all models had more than 10 datasets where the difference between the benchmark evaluation by the original authors and the benchmark evaluation run by me differed by at least 50\%. These high discrepancies typically occurred for datasets with a low anomaly count, making even small variations in detected anomalies strongly noticeable in the scores. When comparing model types, statistical methods generally showed higher consistency between runs than Neural Network-based methods. This is somewhat to be expected, as the inherent complexity, numerous parameters, and sensitivity to initialization or even minor data variations in Neural Networks can lead to greater variability in outcomes, even when attempting to fix seeds. The notable exception here was IForest; its inconsistency was somewhat surprising, especially given it was configured with 200 n\_estimators and a consistent seed, factors which should generally promote stability. Interestingly, Moment (zero-shot) demonstrated remarkably high consistency, outperforming even 3 statistical methods and significantly surpassing most other neural network models. This high consistency is largely attributable to its fundamental zero-shot nature: as a pre-trained model, it involves little re-training randomness in each run, relying instead on fixed, deterministic weights for its inferences. Multiple datasets appeared twice as an entry for the highest absolute difference. These are: \#849, \#865, \#579, \#534.

\subsubsection{Multivariate Results}

Table 3 provides information about the differences found between both benchmark evaluations for multivariate detection models.

The results for multivariate detections models are in many ways similar to those of univariate detections models. Only 5 out of 20 models display an average difference between runs of more than 0.100, with those being TranAD, PCA, OmniAnomaly, USAD, and IForest. 1/4th of all models had more than 10 datasets where the difference between the benchmark evaluation by the original authors and the benchmark evaluation run by me differed by at least 50\%. These high discrepancies typically occurred for datasets with a low anomaly count. As with the univariate models, when comparing model types, statistical methods generally showed higher consistency between runs than Neural Network-based methods. The exceptions here being the statistical models PCA and IForest (ranking 2nd and 5th), and the Neural Network-based model AutoEncoder (ranking 14th). HBOS was the only model to have its largest difference occur during a dataset with multiple hundred anomalies. Given the small absolute difference (0.00000291), this seems insignificant. Most prone to differences was the dataset \#175, appearing 5 times as the dataset with the highest absolute difference, followed by \#156 with 4 appearances. 
\begin{table}[htbp]
    \centering
    \label{tab:vus_pr_diff_summary_multi}
    \begin{tabular}{lrrrrrrr}
        \toprule
        \textbf{Model} & \textbf{\makecell[r]{Avg. Abs.\\Diff.}} & \textbf{\makecell[r]{Max. Abs.\\Diff.}} & \textbf{\makecell[r]{File\\ID}} & \textbf{\makecell[r]{Num. of\\Anom.}} & \textbf{\makecell[r]{Datasets\\>5\% Diff.}} & \textbf{\makecell[r]{Datasets\\>25\% Diff.}} & \textbf{\makecell[r]{Datasets\\>50\% Diff.}} \\
        \midrule
        \rowcolor{myLightBlue} TranAD & 0.185 & 0.910 & 175 & 4 & 83 & 36 & 28 \\
        \rowcolor{lightgray} PCA & 0.155 & 0.855 & 156 & 1 & 91 & 44 & 17 \\
        \rowcolor{myLightBlue} OmniAnomaly & 0.148 & 0.910 & 175 & 4 & 110 & 36 & 11 \\
        \rowcolor{myLightBlue} USAD & 0.147 & 0.909 & 175 & 4 & 109 & 36 & 8 \\
        \rowcolor{lightgray} IForest & 0.122 & 0.942 & 156 & 1 & 85 & 29 & 12 \\
        \rowcolor{myLightBlue} CNN & 0.095 & 0.922 & 187 & 6 & 70 & 15 & 12 \\
        \rowcolor{myLightBlue} LSTMAD & 0.086 & 0.952 & 175 & 4 & 57 & 10 & 6 \\
        \rowcolor{myLightBlue} FITS & 0.065 & 0.479 & 007 & 1 & 64 & 11 & 0 \\
        \rowcolor{myLightBlue} AnomalyTransformer & 0.036 & 0.614 & 175 & 4 & 33 & 5 & 1 \\
        \rowcolor{lightgray} CBLOF & 0.030 & 0.870 & 194 & 5 & 8 & 7 & 5 \\
        \rowcolor{lightgray} KMeansAD & 0.019 & 0.566 & 046 & 1 & 19 & 1 & 1 \\
        \rowcolor{lightgray} RobustPCA & 0.015 & 0.396 & 014 & 1 & 17 & 1 & 0 \\
        \rowcolor{lightgray} EIF & 0.015 & 0.095 & 008 & 2 & 1 & 0 & 0 \\
        \rowcolor{myLightBlue} AutoEncoder & 0.009 & 0.318 & 017 & 1 & 6 & 2 & 0 \\
        \rowcolor{lightgray} KNN & 0.008 & 0.504 & 156 & 1 & 8 & 1 & 1 \\
        \rowcolor{lightgray} MCD & 0.007 & 0.544 & 194 & 5 & 2 & 1 & 1 \\
        \rowcolor{lightgray} LOF & 0.004 & 0.340 & 156 & 1 & 2 & 1 & 0 \\
        \rowcolor{lightgray} OCSVM & 0.004 & 0.370 & 017 & 1 & 2 & 2 & 0 \\
        \rowcolor{lightgray} HBOS & 1.71e-08 & 2.91e-06 & 123 & 463 & 0 & 0 & 0 \\
        \rowcolor{lightgray} COPOD & 9.61e-18 & 2.22e-16 & 008 & 2 & 0 & 0 & 0 \\
        \bottomrule
    \end{tabular}
    \par
    \vspace{1em}
    \noindent 
    \begin{tabular}{@{} p{1em} l @{}} 
        \rowcolor{lightgray} \strut & Statistical Methods \\
        \rowcolor{myLightBlue} \strut & Neural Network-based Method \\
    \end{tabular}
    \caption{\label{tab:Table 3} Summary of absolute differences in VUS-PR for multivariate datasets.}
\end{table}

\subsubsection{Benchmarking Conclusion}

Across both univariate and multivariate anomaly detection models, the observed trends regarding benchmark evaluation consistency are largely parallel. While most models displayed relatively small average differences between the two evaluations, a significant portion exhibited substantial discrepancies on individual datasets, particularly those with low anomaly counts where even minor variations in anomaly detection profoundly impacted scores. A consistent finding was that statistical methods generally demonstrated higher consistency than Neural Network-based models. However, notable exceptions exist, such as IForest often showing surprising inconsistency despite its configuration, and conversely, zero-shot models like Moment achieving remarkable stability due to their pre-trained, less randomizable nature. Given that most of the highest absolute differences can be found on recurring datasets, it is reasonable to assume that the specific type of anomalies and data sequences significantly influences the consistency between benchmarking runs. Furthermore, since a large portion of datasets with equally low anomaly counts are not present on this list, it can be ruled out that the higher differences between runs are entirely attributable solely to their low anomaly count. \par
However, it's important to stress that these observations - for univariate and multivariate models alike - are based on a very limited sample size of just two runs (the original authors' and my own). Therefore, while these results are noteworthy, this comparison doesn't provide any conclusive results regarding the general reproducibility of these models without further, more extensive investigation.

hh

\section{Dataset Creation}
A common hurdle cited amon Time Series Anomaly Detection researchers is the lack of high quality, accurately labeled data \parencites[p.~2]{liu2024elephant}[p.~1-6]{wu2021current}[p.~1]{paparrizos2022tsb}. Out of the existing datasets that fulfill the desired criteria, most are either entirely synthetic or contain artificially inserted anomalies that attempt to mimic real world scenarios based on statistical assumptions. To contribute to the current state of research, I will be providing 4 high quality, real datasets with accurate labels. 

\subsection{Dataset Description}
The provided datasets contain information about RAM utilization during matrix multiplication. While the first 3 datasets each include 1 unique type of anomaly, the fourth dataset combines all 3 anomalies. \par

\textbf{Baseline Load:} All four datasets have the same baseline load for their default state. A script performs continous matrix multiplications, randomly selecting a matrix of size 3000, 3500, 4500, or 5000 and performing 3 multiplications of the selected size. This is follwed by a sleep period of 4 seconds before another set of matrix multiplication resumes. Such a sequence of multiplications follwed by a sleep time will be defined as an "baseline instance" going forward. Each dataset will first run for a predetermined amount of time without any anomalies, which can be used as the training split for semi-supervised models.

\textbf{Dataset 1 - Large Matrix Injection:} For the first dataset, a particularily large matrix multiplication is performed as the anomaly. The generation starts with a large matrix multiplication to prepare the memory and avoid any changes in baseline RAM utilization after the first anomaly injection. 
\begin{itemize}
    \item \textit{Baseload} -- Matrix Size: [3000, 3500, 4500, 5000] | Number of Multiplications: 3 | Sleep Time: 4 sec
    \item \textit{Runtime} -- Total: 90 min | Anomaly-free: 20 min 
    \item \textit{Preparation Matrix} -- Matrix Size: 6000 | Multiplications: 4
    \item \textit{Anomaly} -- Type: Large Matrix Injection | Matrix Size: 6000 | Multiplications: 4 | Frequency: 1/50 baseload instances
\end{itemize}

\textbf{Dataset 2 - Sleep Time Injection:} For the second dataset, an additional sleep time is performed instead of a matrix multiplication. This results in a sequence consisting of: regular sleep time -> anomaly sleep time -> regular sleep time. Given that the anomaly does not require more memory than the baselaod calculations, no preparation is requried at the start of the dataset generation.
\begin{itemize}
    \item \textit{Baseload} -- Matrix Size: [3000, 3500, 4500, 5000] | Number of Multiplications: 3 | Sleep Time: 4 sec
    \item \textit{Runtime} -- Total: 90 min | Anomaly-free: 20 min 
    \item \textit{Anomaly} -- Type: Sleep Time Injection | Sleep Time Duration: 2 sec | Frequency: 1/50 baseload instances
\end{itemize}

\textbf{Dataset 3 - Medium Matrix Injection:} For the third dataset, a matrix multiplication is performed as the anomaly. The size of the matrix falls between the already existing matrices in the baselaod calculations. Given that the anomaly does not require more memory than the baselaod calculations, no preparation is requried at the start of the dataset generation.
\begin{itemize}
    \item \textit{Baseload} -- Matrix Size: [3000, 3500, 4500, 5000] | Number of Multiplications: 3 | Sleep Time: 4 sec
    \item \textit{Runtime} -- Total: 90 min | Anomaly-free: 20 min 
    \item \textit{Preparation Matrix} -- Matrix Size: 4000 | Multiplications: 3
    \item \textit{Anomaly} -- Type: Medium Matrix Injection | Matrix Size: 4000 | Multiplications: 3 | Frequency: 1/50 baseload instances
\end{itemize}

\textbf{Dataset 4 - Combined Anomaly Injection:} For the fourth dataset, all three anomalies are injected. The generation starts with a large matrix multiplication to prepare the memory and avoid any changes in baseline RAM utilization after the first anomaly injection. The same combined anomaly frequency is maintained as in the previous three datasets. The total length of the dataset is adjusted upwards. 
\begin{itemize}
    \item \textit{Baseload} -- Matrix Size: [3000, 3500, 4500, 5000] | Number of Multiplications: 3 | Sleep Time: 4 sec
    \item \textit{Runtime} -- Total: 160 min | Anomaly-free: 25 min 
    \item \textit{Preparation Matrix} -- Matrix Size: 6000 | Multiplications: 4
    \item \textit{Anomaly 1} -- Type: Large Matrix Injection | Matrix Size: 6000 | Multiplications: 4
    \item \textit{Anomaly 2} -- Type: Sleep Time Injection | Sleep Time Duration: 2 sec
    \item \textit{Anomaly 3} -- Type: Medium Matrix Injection | Matrix Size: 4000 | Multiplications: 3 
    \item \textit{Combined Anomaly Frequency} 3
\end{itemize}



\section{Conclusion}

\section{Zitieren und Referenzieren}



\section{Abbildungen}

Abbildungen erfordern das package \textit{graphicx}. 
Idealerweise verwendet man Vektorgrafiken oder hochaufgelöste Bitmaps. 
Eine gute Variante ist das Verwenden von PDFs.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.3\textwidth]{siegel.pdf}
    \caption{Siegel der Universität}
    \label{fig:my_label}
\end{figure}


\section{Tabellen}

Die Tabular-Umgebung gibt die Anzahl Spalten an, deren Orientierung, Breite und evtl. Zwischenlinien. 


\begin{table}[ht]
    \centering
    \caption{Meine Tabelle}
        \begin{tabular}{ cccc } 
        \toprule
        col1 & col2 & col3 \\
        \midrule
        \multirow{3}{4em}{Multiple row} & cell2 & cell3 \\ 
        & cell5 & cell6 \\ 
        & cell8 & cell9 \\ 
        \bottomrule
    \end{tabular}
    \label{tab:countries}
\end{table}

\section{Formeln}

\begin{equation}
    \sum_{i=1}^N x_i
    \label{eq:1}
\end{equation}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Literaturverzeichnis wird 
%% automatisch eingefügt
%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\clearpage
\lhead{}
\printbibliography
\addcontentsline{toc}{section}{\bibname}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Anhang (optional) 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\clearpage
\appendix
\section{Anhang A}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Eidesstattliche Erklärung
%% muss angepasst werden 
%% in Erklaerung.tex
%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\input{Erklaerung.tex}

\end{document}
