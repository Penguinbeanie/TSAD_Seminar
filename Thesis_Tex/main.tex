\documentclass[12pt,oneside]{article}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%   Zusaetzliche Pakete  %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\usepackage{enumerate}  
\usepackage{fancyhdr}
\usepackage{a4wide}
\usepackage{graphicx}
\usepackage{palatino}
\usepackage{multirow}
\usepackage{booktabs}
\usepackage{titlesec}
\usepackage{float}
\usepackage[table]{xcolor}
\usepackage{amssymb}
\usepackage{xcolor}
\usepackage[utf8]{inputenc}

\usepackage{enumitem}% http://ctan.org/pkg/enumitem

%folgende Zeile auskommentieren für englische Arbeiten
%\usepackage[ngerman]{babel}
%folgende Zeile auskommentieren für deutsche Arbeiten
\usepackage[english]{babel}

\usepackage[T1]{fontenc}
\usepackage{paratype}\renewcommand{\familydefault}{\sfdefault}
\usepackage[utf8]{inputenc}
\usepackage[bookmarks]{hyperref}
\usepackage[justification=centering]{caption}
\usepackage[style=authoryear,natbib=true,backend=biber,maxbibnames=20]{biblatex}
\usepackage{csquotes}
\usepackage{booktabs}
\usepackage{makecell}
\usepackage{listings}
\bibliography{literatur}

\setlength{\parindent}{0em} 
\setlist[itemize]{noitemsep, topsep=0pt}
\setlist[enumerate]{noitemsep, topsep=0pt}

\colorlet{myLightGreen}{green!10!gray} 
\colorlet{myLightBlue}{blue!20}   

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Definition der Kopfzeile %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\pagestyle{fancy}
\fancyhf{}
\cfoot{\thepage}
\setlength{\headheight}{16pt}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%  Definition des Deckblattes und der Titelseite  %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\newcommand{\JMUTitle}[9]{

  \thispagestyle{empty}
  \vspace*{\stretch{1}}
  {\parindent0cm
  \rule{\linewidth}{.7ex}
  }
  \begin{flushright}
    \sffamily\bfseries\Huge
    #1\\
    \vspace*{\stretch{1}}
    \sffamily\bfseries\large
    #2\\
    \vspace*{\stretch{1}}
    \sffamily\bfseries\small
    #3
  \end{flushright}
  \rule{\linewidth}{.7ex}

  \vspace*{\stretch{1}}
  \begin{center}
    \includegraphics[width=2in]{siegel} \\
    \vspace*{\stretch{1}}
    \Large #5 \\

    \vspace*{\stretch{2}}
   \large Lehrstuhl f\"{u}r Wirtschaftsinformatik und Business Analytics\\
    \large Universität Würzburg\\
    \vspace*{\stretch{1}}
    \large Betreuer:  #8 \\[1mm]
    \large Assistent:  #9 \\[1mm]
    \vspace*{\stretch{1}}
    \large #6, den #7
  \end{center}
}

\newcommand{\supref}[1]{\textsuperscript{\footnotesize{\ref{#1}}}}

\titlespacing*{\section}
{0pt}{3.5ex plus 1ex minus .2ex}{.2ex plus .2ex}
\titlespacing*{\subsection}
{0pt}{1.5ex plus 1ex minus .2ex}{.2ex plus .2ex}
\titlespacing*{\subsubsection}
{0pt}{1.5ex plus 1ex minus .2ex}{.2ex plus .2ex}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%  Beginn des Dokuments  %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}
  \JMUTitle
      {Time Series Anomaly Detection Benchmarking}        % Titel der Arbeit
      {Philip Spaier}                        % Vor- und Nachname des Autors
      {3110375}
      
      {Seminararbeit } % Art der Arbeit
      {Würzburg}                           % Ort der Erstellung
      {05.04.2025}                          % Tag der Abgabe
      {Prof. Dr. Gunther Gust}           % Name des Erstgutachters
      {Viet Nguyen} % Name des/der betreuenden Assistent/Assistentin
      
  \clearpage

\lhead{}
\pagenumbering{Roman} 
    \setcounter{page}{1}

\tableofcontents
\clearpage

\addcontentsline{toc}{section}{\listfigurename}
\listoffigures

\addcontentsline{toc}{section}{\listtablename}
\listoftables
\clearpage

\setlength{\parskip}{0.5em} 


%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%  Kurzzusammenfassung   %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\lhead{Abstract}
\section*{Abstract}

Eine Kurzzusammenfassung der Vorgehensweise und der wesentlichen Ergebnisse.

Allgemeine Merkmale
\begin{itemize}
    \item Objektivität: Es soll sich jeder persönlichen Wertung enthalten.
    \item Kürze: Es soll so kurz wie möglich sein.
    \item Verständlichkeit: Es weist eine klare, nachvollziehbare Sprache und Struktur auf.
    \item Vollständigkeit: Alle wesentlichen Sachverhalte sollen enthalten sein.
    \item Genauigkeit: Es soll genau die Inhalte und die Meinung der Originalarbeit wiedergeben.
\end{itemize}{}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%  Einstellungen  %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\clearpage
\pagenumbering{arabic}  
    \setcounter{page}{1}
\lhead{\nouppercase{\leftmark}}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%  Hauptteil  %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\section{Literature Review} \label{einleitung}
Time Series Anomaly Detection (TSAD), as a subcategory of the broader field of Anomaly Detection, has seen increased attention since the start of the twenty first century. With the internet having established itself as a persistent and omnipresent force in every imaginable aspect of human life, time series data can be found in abundance. Modern developments in Internet-of-Things (IoT) applications, the digitization of financial data, and a massive rise in the consumption of streaming services have contributed to an exponential growth of time series data [source needed]. This in turn has made the manual search of potential anomalies in many fields completely infeasible, leading to an increased demand for automated anomaly detection methods. While there is a continuously growing repertoire of such automated detection methods, the lack of a generally accepted and reliable benchmark makes not just further developments but also the selection of appropriate models difficult. In the following sections of this literature review, I will provide the reader with a better understanding of context independent Time Series Anomaly Detection, the most commonly applied methods, and the current state of benchmarking.


\subsection{Time Series Data and Anomaly Detection Definition}
Time Series Data, as used in the rest of this thesis, shall be defined as follows: a sequence of data or observations, typically indexed by or associated with specific timestamps, collected in chronological order over a period of time.
For the purpose of analysis, continuous signals must be converted into individual data points. Each datapoint can either represent a binary state (1 or 0), be a numerical value measured on a ratio scale (eg. number of occurrences), or a numerical value measured on an interval scale (eg. temperature on a Celsius scale).
A time series with a dimensionality of one (only a single feature) will be referred to as "univariate", while a time series with higher dimensionality (multiple features) will be referred to as "multivariate". \\
An anomaly will be defined as follows: an abnormal, rarely occurring data point or sequence, that has to be be detectable with exclusively context independent methods. Individual anomalous data points will be referred to as "point based" anomalies. Multiple consecutive anomalous points, each of which might be unremarkable on their own, while displaying unusual behavior as a sequence,  will be referred to as "sequence based" or "collective" anomalies \parencites[p.~3]{liu2024elephant}[p.~8]{chalapathy2019deeplearninganomalydetection}. A separate category of anomalies would be context dependent ones. Those are data points or sequences, possibly indistinguishable from normal ones if analyzed without context, but if combined with additional information about the field or time series, are considered anomalous \citep[pp.~7-8]{chalapathy2019deeplearninganomalydetection}. Context dependent anomalies will not be topic of the research presented here. \\
Given those definitions, Time Series Anomaly Detection is therefore the task of correctly and autonomously identifying anomalies within a given time series.

Insert Images of point vs sequence.

\subsection{Relevant Fields}

The following is an overview of fields relying on Time Series Anomaly Detection. It is a non-exhaustive list, simply highlighting some of the most prominent use cases to provide context. \

\textbf{Illicit Activity and Fraud Detection:} With the global financial system relying primarily on digital transactions, it has become crucial to detect fraudulent activities as quickly and accurately as possible. A particularly obvious example is credit card fraud, creating an estimated yearly loss in the billions of dollar \parencites[p.~2]{FinFraud}. Companies like Visa and Mastercard put great emphasis on being able to detect anomalous transactions in real time to then analyses them and prevent potential harm to their customers \citep{VisaFr}. While credit card fraud is a prominent application, the scope of financial anomaly detection extends significantly further, playing a critical role in the operations of stock exchanges, brokerage firms, and banks. These institutions leverage anomaly detection techniques to identify various illicit activities, ensure market integrity, manage operational risks, and comply with stringent regulatory requirements \citep{DBSurv}. \

\textbf{Healthcare:} Healthcare critically relies on analyzing physiological signals, such as those captured by the electrocardiogram (ECG), which provides vital time series data reflecting the heart's electrical activity. While historically, ECG analysis has focused on identifying established patterns of known heart diseases, this approach often fails to detect rare or atypical anomalies that do not fit predefined categories, potentially missing critical conditions. To address this issue, Time Series Anomaly Detection has been introduced for the purpose of detecting such rare anomalies that would go unnoticed by conventional pattern classification \parencites[p.~1-2]{jiang2024anomalydetectionelectrocardiogramsadvancing}.

\textbf{Website Traffic:} A common threat faced by web-services are so called Denial or Service (DoS) and Distributed Denial of Service (DDoS) attacks. These include hitting a webserver with so many requests that the systems becomes inoperational and can no longer service legitimate users \citep{BSI1}. A significant challenge in detecting these attacks is that the malicious traffic can often mimic normal network traffic, making it difficult for traditional packet-based intrusion detection systems or statistical methods reliant on fixed thresholds to accurately identify attacks, especially when they are hidden within legitimate flows. Time series analysis allows systems to observe and distinguish the instant changes in network traffic that indicate an attack, even when individual packets or simple statistics are insufficient. Time series anomaly detection provides a means to autonomously identify and localize potentially harmful deviations within the network traffic and thereby ensure the availability and reliability of services \parencites[pp.~1-2]{FOULADI2020102587}. \par
The list extends far beyond the fields named above. Time Series Anomaly Detection can be also found in astronomy \citep{astro}, earth sciences, manufacturing \parencites[p.~1]{Zamanzadeh_Darban_2024}, cybersecurity, and law enforcement \parencites[p.~1]{boniol2024divetimeseriesanomalydetection}.


\subsection{Detection Methods}
Detection methods, in common descriptions and within the scientific literature alike, are often grouped or distinguished by a variety of aspects. This categorization can sometimes lack a consistent taxonomy. To provide a clearer framework, I will now systematically explain and categorize these methods through three key perspectives:
\begin{itemize}
    \item Degree of supervision
    \item Architecture
    \item Technique
\end{itemize}

\subsubsection{Degree of Supervision} \textit{Unsupervised models} operate on data without any explicit labels distinguishing normal from anomalous instances. While they don't require pre-labeled data, they typically do require a training or fitting phase. During this phase, the model learns the inherent structure, patterns, distributions, or densities from the unlabeled dataset.\par \textit{Semi-supervised models} are trained exclusively on data that is known or assumed to be 'normal.' They do not require labeled anomalies for training. The model learns a precise representation or boundary of this normal behavior. During deployment, any new data instance that significantly deviates from this learned model of normalcy is flagged as an anomaly. \par \textit{Supervised models} require a dataset where both normal and anomalous instances are explicitly labeled beforehand. The model is then trained to learn the distinguishing features or decision boundaries that separate these classes, effectively treating anomaly detection as a (often highly imbalanced) classification problem. \parencites[pp.~5-6]{boniol2024divetimeseriesanomalydetection}[p.~3]{liu2024elephant}[p.~3-4]{SchmidlEtAl2022Anomaly}.

\subsubsection{Architecture} \textit{Statistical models} identify anomalies by relying on statistical assumptions to detect deviations from expected data distributions. They often involve fitting a distribution model to the data and measuring abnormality based on probabilities or distances from the calculated distribution. Statistical models often require a threshold to be set beforehand\parencites[p.~6-7]{liu2024elephant}[p.~1]{FOULADI2020102587}. \par \textit{Neural Network based models} are a collection of distributed, adaptive, non-linear processing units with adjustable weights \parencites[p.~427]{GURESEN2011426}. They rely on a training dataset and are often semi-supervised. Deep neural networks, a subcategory of neural networks, model spacial and temporal dependencies \parencites[p.~6-7]{liu2024elephant}[p.~6]{Zamanzadeh_Darban_2024}.\par \textit{Foundational Models} utilize transfer learning, using knowledge from a different class of tasks and then applying it on the target task. These models are pre-trained and are then being fine-tuned \parencites[p.~4]{bommasani2022opportunitiesrisksfoundationmodels}. In the context of TSAD, those models are GPT models fine tuned on time series data, general purpose time series models, or originally time series classification models now used for anomaly detection \parencites[p.~7]{liu2024elephant}.

\subsubsection{Technique} \textit{Distance based models} work on the idea that anomalous points or sequences will further away when using a distance measurement. They can be either be compared to their nearest neighbor, all other points/subsequences, or cluster centers \parencites[p.~6]{SchmidlEtAl2022Anomaly}. Such distances are calculated in various ways depending on the model and implementation, with the most common definitions being the Euclidean distance or the Z-normalized Euclidean distance. Distance based models use only the x- and y-axis data, with no labels being required \parencites[p.~8]{boniol2024divetimeseriesanomalydetection}. \par \textit{Forecasting models} learn the normal patters of a time series and, often using a sliding context window, forecast the next datapoint in the series. The forecasted and actual data points are then compared, with the difference being used for an anomaly score. Given a high enough anomaly score, a point is considered an anomaly. Such models are usually semi-supervised \parencites[p.~4-5]{SchmidlEtAl2022Anomaly}. \par \textit{Isolation Tree Models} use ensembles of random trees, selecting random features and splits, to separate points or sequences from each other. It operates on the idea that anomalies require fewer steps to be separated from the rest of the data than normal points/sequences. For each point/sequence, the distance from the root is calculated. The shorter a distance is, the more likely is a point/sequence to be an anomaly. These models can be both unsupervised and supervised \parencites[p.~6-7]{SchmidlEtAl2022Anomaly} \par \textit{Distribution based models} estimate a distribution of the time series and then score individual points or sequences as anomalous or normal based on it. Anomalous points are expected to have a low probability. Alternatively to probabilities, the anomaly score can also be calculated using likelihoods or distances. These models are generally unsupervised or occasionally semi-supervised \parencites[p.~6]{SchmidlEtAl2022Anomaly}. \par \textit{Graph based models} methods turn time series data, or parts of it, into a graph structure. This graph represents the different types of patterns (subsequences) found in the data as nodes, and how these patterns follow each other over time as connections (edges) between the nodes. Anomalies are then determined based on usual structures or behaviors found in the graph \parencites[p.~23-24]{boniol2024divetimeseriesanomalydetection}. Graph based time series models can be further divided in multiple subcategories, including AutoEncoder- and GAN-based methods, as well as predictive graph models \parencite{ho2025graphanomalydetectiontime}. \par \textit{Reconstruction models} learn a time series' features and patterns by encoding normal data into a low dimensional space. Given a test dataset, they compress test data and reconstruct it using their model based on that low-dimensional space. Should a point or sequence of this reconstructed version deviate substantially from the actual data, then it is labeled as anomalous. These models are often considered semi-supervised because they typically use normal labeled data for training. However, models that do not rely on a training dataset and instead directly encode and reconstruct the test data also exist, operating in an unsupervised manner. \parencites[p.~5]{SchmidlEtAl2022Anomaly}. \par \textit{Encoder based models} operate similarly to reconstruction models. They compress a given time series into a low-dimensional representation, but instead of reconstructing it, they directly compare this compressed version to their model of normal time series. Anomalous points or sequences might have unusual encoded representations, and their deviations from the normal model are then used to calculate an anomaly score \parencites[p.~5-6]{SchmidlEtAl2022Anomaly}.

\subsection{Performance Metrics}

For the effective evaluation of a models performance, as defined by \cite{papaVus}, metrics have to fullfil the following criteria:
\begin{itemize}
    \item \textit{Robustness to Lag:} The evaluation measure should be insensitive to slight temporal shifts or lags in anomaly scores.
    \item \textit{Robustness to Noise:} The evaluation measure should be stable and unaffected by noise in the anomaly scores.
    \item \textit{Robustness to Anomaly Cardinality Ratio:} The evaluation measure's score should not be influenced by the proportion of anomalies in the data.
    \item \textit{High Separability between Accurate and Inaccurate Methods:} The measure must effectively distinguish between accurate and inaccurate detection methods.
    \item \textit{Consistency:} The measure should produce repeatable scores for similar data and consistently rank different methods.
\end{itemize}
Commonly applied performance measures for TSAD can generally be classified based on two characteristics: Point-wise or Range-wise, and Threshold-dependent or Threshold-independent.


\subsubsection{Point-wise or Range-wise}
\textit{Point-wise evaluation measures} look at each anomalous point independently, determining in a binary fashion whether a model classified them correctly as normal or anomalous \parencite[p.~7]{liu2024elephant}. These measures suffer from a variety of issues. Most crucially, they can unfairly penalize methods that detect only part of an anomalous range or whose detection peak doesn't perfectly align with the labeled range. Further more, they are sensitive to temporal lag. Should an anomalous data point be detected slightly before or after the actual anomaly occurs, a fully point-wise metric will score it with an unreasonably low score \parencites[p.~2778]{papaVus}.\\ \textit{Range-wise measures} look at anomalies not just from the perspective of individual points but take sequences into consideration. For anomalous sequences, their evaluation can involve determining how much the detected and the actual sequence overlap. Additionally, such measures may incorporate strategies like adequately handling lag (e.g., by considering an anomaly detected if it's within a specified range of an actual one, even if not at the exact spot) or including a cardinality factor to penalize models that incorrectly segment anomalies (such as detecting multiple short ones for a single large event, or vice-versa) \parencite[p.~7]{liu2024elephant}.
\subsubsection{Threshold-dependent or Threshold-independent}
\textit{Threshold-dependent measures} require a threshold to be set that determines whether an anomaly score classifies a value as anomalous or normal. This can be done based on statistical assumptions, or using dynamic algorithms that adjust to the data and results \parencites[p.~38-39]{boniol2024divetimeseriesanomalydetection}. Setting these thresholds automatically, however, is often difficult when working with large and diverse datasets, and the chosen thresholds can drastically change a metric's accuracy. Noise and the normal-to-anomalous ratio in a time series can be particularly problematic \parencites[p.~2777-2778]{papaVus}. \\
\textit{Threshold-independent measures} evaluate the performance of a time series anomaly detection method without needing a specific score cutoff to decide what constitutes an anomaly. Instead of relying on a fixed threshold, they assess how effectively the method's anomaly scores rank true anomalies higher than normal data points across the entire range of scores \parencites[p.~39-41]{boniol2024divetimeseriesanomalydetection}.

\subsubsection{Definition and Classifgication of Metrics}

The following defines the most commonly used metrics and classifies them into the above described categories \parencites[p.2776-2780]{papaVus}:
\begin{itemize}
    \item \textit{Precision / Range Precision:} number of correctly identified anomalies over all anomalies.
    \item \textit{Recall (TPR) / Range Recall:} number of correctly identified anomalies over all anomalies.
    \item \textit{F-Score / Range F-Score:} Harmonic Mean of Precision and Recall. 
    \item \textit{False Positive Rate (FPR):} number of points wrongly identified as anomalies over the total number of normal points.
    \item \textit{AUC-ROC:} area under the curve corresponding to TPR on the y-axis and FPR on the x-axis at all threshold levels.
    \item \textit{AUC-Precision:} area under the curve corresponding to the Recall on the x-axis and Precision on the y-axis at all threshold levels.
    \item \textit{VUS-ROC:} generating multiple ROC curves for a range of different buffer lengths. These stacked ROC curves form a 3D surface, and VUS-ROC is the volume beneath this surface.
    \item \textit{VUS-Precision:} generating multiple Precision-Recall curves for a range of different buffer lengths. These stacked PR curves form a 3D surface, and VUS-Precision (VUS-PR) is the volume beneath this surface.
\end{itemize}




\begin{table}[htbp]
  \centering
  \label{tab:evaluation_categories}

  \begin{tabular}{lcc}
    \toprule
    & Threshold-dependent & Threshold-independent \\
    \midrule
    Point-wise & \makecell[l]{Precision \\ Recall \\ False Positive Rate \\ F-Score} & \makecell[l]{AUC-ROC \\ AUC-Precision} \\
    \midrule
    Range-wise & \makecell[l]{Range Precision \\ Range Recall \\ Range F-Score} & \makecell[l]{VUS-ROC \\ VUS-Precision} \\
    \bottomrule
  \end{tabular}
  \caption{Evaluation Measures}

\end{table}

\textbf{}


\subsection{State of Benchmarking}

While time series anomaly detection is a well-established field, most advancements in systematic bench marking have been made within the last decade. The following will provide an overview over the most important papers and datasets contributing to this endeavor. As always, given the large corpus of work, this is not an exhaustive list.  \par
\textbf{Yahoo (2015)}: Yahoo provides one of the earliest available labeled large scale TSAD datasets. It consists of real data with time series from various Yahoo services and synthetic data, containing trends, noise, and seasonality \parencite{Yahoo}. \par
\textbf{Numenta Anomaly Benchmark (NAB), 2015:} The Numeta Anomaly Benchmark is often considered to be the first large scale open source benchmarking environment for TSAD. At the time of release, it contained 58 datasets, made up of a mixture of artificial and real time series. The labels are first created by multiple humans, then combined into a ground truth by an algorithm. NAB uses a custom threshold dependent scoring function for the evaluation of a model's performance, designating high value to an algorithms ability to detect an anomaly as early as possible. It is designed specifically for real time anomaly detection, not static analysis. Therefore, only unsupervised models can be tested, with no training/test split of the data \parencites{Lavin_2015}.\par
\textbf{Illusion of Progress (2021):} \cite{wu2021current} provide substantial criticism regarding the state of TSAD benchmarking. The authors find most previously created datasets to contain one or multiple of the following flaws:
\begin{itemize}
    \item \textit{Triviality:} Many anomalies are "are so simple that solving them seems pointless or even absurd" \parencites[p.~2]{wu2021current}.
    \item \textit{Unrealistic Anomaly Density:} Many time series have anomaly rates so high that they can realistically no longer be defined as anomalous. The task turns into a classification problem.
    \item \textit{Mislabeled Ground Truth:} Many time series have data that, without context, appears to be mislabeled. Normal data points are falsely labeled as anomalies and vice versa.
    \item \textit{Run-to-failure Bias:} In the case of real data, many systems are operated until failure. This results in an unusually high anomaly count towards the end of the dataset.
\end{itemize}
The authors introduce their UCR Time Series Anomaly Archive, a collection of 250 curated univariate time series from human medicine, biology, meteorology and industry to provide a dataset that combats these issues \parencite{wu2021current}. \par
\textbf{Exathlon (2021):} \cite{jacob2021exathlonbenchmarkexplainableanomaly} introduce the first public benchmarking suite for multivariate time series anomaly detection. Their dataset consists of time series collected 100 executions of 10 distributed streaming jobs on a Spark cluster. The datasets contain primarily sequence based anomalies; the tested models are primarily semi-supervised. Exathlon evaluates detection performance (Precision, Recall, F-Score, and AUPRC) and computational efficiency. \par
\textbf{TODS (2021):} \cite{NEURIPS} contribute crucially to the taxonomy for outliers and synthetic anomaly injection. 'Point-wise' outliers. including their subcategories 'global outliers' and 'contextual outliers', as well as 'pattern-wise' outliers, including their subcategories 'shapelet outliers', 'seasonal outliers', and 'trend outliers' are introduced. The authors provide 35 new synthetic datasets and 4 new multivariate real datasets, in addition to 9 existing datasets for their benchmark. \par
\textbf{GutenTAG (2022):} \cite{SchmidlEtAl2022Anomaly} implement and evaluate 71 different algorithms on 976 time series across a variety of fields, both univariate and multivariate. Furthermore, they introduced the GutenTAG synthetic dataset generator, allowing the creation of time series with with different lengths, variances, amplitudes, frequencies, and dimensions. \par
\textbf{TSD-UAD (2022):} \cite{paparrizos2022tsb} introduce TSB-UAD, a new comprehensive end-to-end benchmark suite designed for evaluating univariate TSAD methods. The benchmark aims to address limitations in current practices, such as the reliance on biased proprietary/synthetic data or limited public datasets. TSB-UAD provides a reproducible platform for researchers by collecting, processing, and formatting a large and diverse set of time series with labeled anomalies.
The TSB-UAD suite encompasses 13,766 univariate time series across various domains, featuring high variability in anomaly types (point, contextual, collective), ratios, and sizes. It includes 18 previously proposed public datasets and contributes two new collections: 126 "artificial" datasets derived from transforming time-series classification data (leveraging the UCR Archive) and 92 "synthetic" datasets generated by applying various global, local, and subsequence transformations to public data to introduce new anomalies and increase detection difficulty. The benchmark suite also provides a Python library to handle pre-processing, post-processing, data generation, transformation, and includes statistical analysis methods (Friedman, Nemenyi tests) for comparing algorithms. It evaluates 12 representative AD methods and introduces measures (Relative Contrast (RC), Normalized Clusteredness of Abnormal Points (NC), Normalized Adjacency of Normal/Abnormal Cluster (NA)) to quantify dataset difficulty. Data and code are made publicly available in a Github repository. \par
\textbf{TSD-AD (2024):} TSB-AD is presented as a new comprehensive benchmark suite for univariate and multivariate time-series anomaly detection (TSAD), designed to address limitations in existing evaluations stemming from flawed datasets, unreliable measures, and inconsistent practices. The benchmark offers 1070 high-quality, curated time series derived from 40 diverse public datasets, substantially increasing the scale and integrity of available data for benchmarking. TSB-AD includes 40 representative AD algorithms spanning statistical, neural network, and foundation model categories. The paper identifies VUS-PR as a robust and reliable evaluation metric, contrasting it with traditional measures prone to biases like Point Adjustment and sensitivity to lag. TSB-AD is released open-source to provide a stable platform for research and establish a leaderboard \parencites{liu2024elephant}.

When looking at the evaluation results of all major benchmarks, it is difficult to point to any specific model as the conclusively best. Given the constantly evolving benchmarking criteria and evolution of dataset quality, this is not surprising. Overall, traditional models have been found across multiple papers and benchmarks to rival, or in many case outperform, newer more complex architectures. Foundational models are promising for point-wise anomalies but get beaten decisively for sequence-wise anomalies. [insert sources]

\section{Dataset Analysis}

Considering how much of the new benchmarking research is based on the assumption that previous datasets are flawed, I have conducted a short qualitative evaluation of the dataset quality found in the NAB benchmark \parencite{Lavin_2015}. As a general guideline, I will be referencing \cite{wu2021current} and their described categories of issues. In all four figures, the red dots are data points labeled as anomalies. 


A sizable portion of all anomalies found in the NAB dataset fit into the category of \textbf{trivial anomalies} and can be detected using simple thresholding functions. Examples of that can be found in Figure 1 \supref{fig:trivial}. \par

\begin{figure}[htbp] 
    \centering 
    \includegraphics[width=\textwidth]{trivial.png}
    \caption{NAB Trivial}
    \label{fig:trivial}
\end{figure}


Equally common are \textbf{mislabeled anomalies}. Examples can be found in Figure 2 \supref{fig:mislab}. The dataset at the time labels two points as anomalies, while completely ignoring the obvious peaks, which are unusual even when comparing with the entirety of the dataset. The time series at the bottom only highlights one seemingly normal datapoint, while again ignoring the unusual peak directly to its right.

\begin{figure}[htbp] 
    \centering 
    \includegraphics[width=\textwidth]{mislab.png}
    \caption{NAB Mislabeled}
    \label{fig:mislab}
\end{figure}

A subcategory of mislabeled anomalies are sequence-based anomalies being labeled as only a single point. Examples can be found in Figure 3 \supref{fig:point_seq}. In the upper time series only the first point of an unusual flatline-like sequence is labeled as an anomaly. In the lower time series only the first point of an elevated sequence is labeled as an anomaly. The time series in the middle highlights only a single point out of an entire sequence as an anomaly. Without context, it is hard to understand why the rest of the sequence should be considered as normal and only this one specific point is anomalous. In such cases, \cite[p.~4-5]{wu2021current} suggests to label the entire sequence as an anomaly.

\begin{figure}[htbp] 
    \centering 
    \includegraphics[width=\textwidth]{point_seq.png}
    \caption{NAB Mislabeled | Point Instead of Sequence}
    \label{fig:point_seq}
\end{figure}

Not as commonly found as the previous two flaws but still present are datasets with a \textbf{run-to-failure bias}, as described by \cite{wu2021current}. Examples of those can be found in Figure 4 \supref{fig:run_to_failure}. In both cases, the time series end on an unusual pattern, representing hardware failures before the systems stop being monitored, These are rightfully marked as anomalies but can introduce a bias towards models that simply flag the end of each time series as an anomaly.

\begin{figure}[htbp] 
    \centering 
    \includegraphics[width=\textwidth]{run_to_failure.png}
    \caption{NAB Run-To-Failure Bias}
    \label{fig:run_to_failure}
\end{figure}

These are not cherry-picked results and can be found throughout the entire NAB dataset. Interestingly enough, the problem where only a single point of an unusual sequence is labeled as an anomaly can even be found in some of the artificially created time series.This brief analysis confirms the findings of previous authors and supports their claims that many of the early TSAD datasets were deeply flawed and to some extent even unusable for their intended purpose.


\section{Replication of TSB-AD Benchmark Results}

\subsection{Setup and Implementation}
For the recreation of the TSB-AD benchmark results, the following core components were used:
\begin{itemize}
    \item AMD Ryzen 5 7500F, 6C/12T, 3.70-5.00GHz
    \item Nvidia 3060TI 8 GB GDDR6 VRAM
    \item 32 GB DDR5 RAM
    \item OS: Windows 11
\end{itemize}

For the actual implementation, the TSB-AD Github repository was cloned and instructions provided by the authors on how to set up the correct development environment were followed. Given the high number of datasets, the benchmark was performed primarily running 3 models in parallel. Two non-resource intensive models using only the CPU (Statistical Methods as classified by the authors of TSB-AD) and one GPU utilizing model (Neural Network-based Method or Foundation Model-based Method). GPU utilization would frequently reach 100\%, whereas CPU utilization rarely even exceeded 60\%.\par
Despite extensive efforts, I was not able to get all provided anomaly detection models to run reliably. These are the models which I could not successfully implement:

\begin{itemize}
    \item \textbf{MomentFM, TimesFM, and TimesFT} require packages that cause unresolvable dependency incompatibilities with TSB-AD.
    \item \textbf{Chronos and OFA} have required packages that do not work on my system for an unidentified reason. 
    \item \textbf{Lag-Lama} has a required file which I can not find in the provided Github repository.
    \item \textbf{EIF} causes the system to run out of memory and crash.
    \item \textbf{Donut} creates a for me unidentifiable error, seemingly stemming from the donut.py file itself.
    \item \textbf{NORMA} and Series2Graph are commercially licensed models. 
\end{itemize}

\subsection{Results}
Since \cite{liu2024elephant} only publish the full VUS-PR results for each individual dataset and model, the comparison will focus on this metric.
\subsubsection{Univariate Results}

\begin{table}[htbp]
    \centering
    \label{tab:vus_pr_diff_summary_uni}
    \begin{tabular}{lrrrrrrr}
        \toprule
        \textbf{Model} & \textbf{\makecell[r]{Avg. Abs.\\Diff.}} & \textbf{\makecell[r]{Max. Abs.\\Diff.}} & \textbf{\makecell[r]{File\\ID}} & \textbf{\makecell[r]{Num. of\\Anom.}} & \textbf{\makecell[r]{Datasets\\>5\% Diff.}} & \textbf{\makecell[r]{Datasets\\>25\% Diff.}} & \textbf{\makecell[r]{Datasets\\>50\% Diff.}} \\
        \midrule
        \rowcolor{lightgray} Sub-IForest & 0.231 & 0.966 & 405 & 1 & 210 & 115 & 67 \\
        \rowcolor{lightgray} Sub-LOF & 0.177 & 0.960 & 603 & 1 & 208 & 89 & 38 \\
        \rowcolor{lightgray} IForest & 0.163 & 0.999 & 552 & 1 & 195 & 81 & 24 \\
        \rowcolor{myLightBlue} LSTMAD & 0.108 & 0.951 & 849 & 2 & 118 & 36 & 12 \\
        \rowcolor{myLightBlue} USAD & 0.106 & 0.982 & 849 & 2 & 111 & 44 & 27 \\
        \rowcolor{myLightBlue} TranAD & 0.093 & 0.934 & 865 & 2 & 109 & 47 & 19 \\
        \rowcolor{myLightBlue} OmniAnomaly & 0.082 & 0.966 & 865 & 2 & 101 & 44 & 13 \\
        \rowcolor{myLightBlue} CNN & 0.079 & 0.987 & 645 & 1 & 125 & 29 & 12 \\
        \rowcolor{myLightBlue} AnomalyTrans & 0.053 & 0.603 & 191 & 2 & 26 & 10 & 5 \\
        \rowcolor{lightgray} KShapeAD & 0.048 & 0.999 & 780 & 1 & 71 & 14 & 8 \\
        \rowcolor{lightgray} SAND & 0.047 & 0.994 & 680 & 1 & 70 & 17 & 7 \\
        \rowcolor{myLightBlue} FITS & 0.038 & 0.450 & 579 & 1 & 86 & 6 & 0 \\
        \rowcolor{lightgray} KMeansAD & 0.012 & 0.268 & 419 & 1 & 21 & 1 & 0 \\
        \rowcolor{lightgray} Sub-MCD & 0.009 & 0.316 & 788 & 1 & 20 & 1 & 0 \\
        \rowcolor{myLightGreen} MOMENT-ZS & 0.007 & 0.413 & 579 & 1 & 9 & 1 & 0 \\
        \rowcolor{myLightBlue} AutoEncoder & 0.006 & 0.152 & 658 & 1 & 8 & 0 & 0 \\
        \rowcolor{lightgray} LOF & 0.002 & 0.075 & 811 & 3 & 2 & 0 & 0 \\
        \rowcolor{lightgray} Sub-KNN & 0.002 & 0.216 & 538 & 3 & 3 & 0 & 0 \\
        \rowcolor{lightgray} POLY & 0.001 & 0.288 & 534 & 1 & 2 & 1 & 0 \\
        \rowcolor{lightgray} Sub-PCA & 0.001 & 0.071 & 534 & 1 & 1 & 0 & 0 \\
        \rowcolor{lightgray} Sub-HBOS & 0.001 & 0.094 & 531 & 1 & 1 & 0 & 0 \\
        \rowcolor{lightgray} Sub-OCSVM & 0.001 & 0.082 & 534 & 1 & 2 & 0 & 0 \\
        \rowcolor{lightgray} MatrixProfile & 5.96e-05 & 0.015 & 017 & 1 & 0 & 0 & 0 \\
        \rowcolor{lightgray} SR & 8.24e-06 & 0.002 & 534 & 1 & 0 & 0 & 0 \\
        \bottomrule
    \end{tabular}
    \par
    \vspace{1em}
    \noindent 
    \begin{tabular}{@{} p{1em} l @{}} 
        \rowcolor{lightgray} \strut & Statistical Methods \\
        \rowcolor{myLightBlue} \strut & Neural Network-based Method \\
        \rowcolor{myLightGreen} \strut & Foundation Model-based Method \\
        
    \end{tabular}
    \caption{\label{tab:Table 2} Summary of absolute differences in VUS-PR for univariate datasets.}
\end{table}

Table 2 provides information about the differences found between both benchmark evalu-
ations for univariate detection models. \par
For most models, the average difference stayed firmly below 0.100, with (Sub-)IForest, Sub-LOF, LSTMAD, and USAD being the main exceptions. Beyond these averages, almost half of all models had at least one dataset where the absolute score differed by more than 0.950. Furthermore, a third of all models had more than 10 datasets where the difference between the benchmark evaluation by the original authors and the benchmark evaluation run by me differed by at least 50\%. These high discrepancies typically occurred for datasets with a low anomaly count, making even small variations in detected anomalies strongly noticeable in the scores. When comparing model types, statistical methods generally showed higher consistency between runs than Neural Network-based methods. This is somewhat to be expected, as the inherent complexity, numerous parameters, and sensitivity to initialization or even minor data variations in Neural Networks can lead to greater variability in outcomes, even when attempting to fix seeds. The notable exceptions here were (Sub-)IForest and Sub-LOF. Their inconsistency were somewhat surprising, especially given that (Sub-)IForest were configured with (150) 200 n\_estimators and a consistent seed, factors which should generally promote stability. Interestingly, Moment (zero-shot) demonstrated remarkably high consistency, outperforming even 3 statistical methods and significantly surpassing most other neural network models. This high consistency is largely attributable to its fundamental zero-shot nature: as a pre-trained model, it involves little re-training randomness in each run, relying instead on fixed, deterministic weights for its inferences. The dataset \#534 appeared 4 times. Multiple datasets appeared twice as an entry for the highest absolute difference. These are: \#849, \#865, \#603, \#579.

\subsubsection{Multivariate Results}

Table 3 provides information about the differences found between both benchmark evaluations for multivariate detection models.

The results for multivariate detection models are in many ways similar to those of univariate detection models. Only 5 out of 20 models display an average difference between runs of more than 0.100, with those being TranAD, PCA, OmniAnomaly, USAD, and IForest. 1/4th of all models had more than 10 datasets where the difference between the benchmark evaluation by the original authors and the benchmark evaluation run by me differed by at least 50\%. These high discrepancies typically occurred for datasets with a low anomaly count. As with the univariate models, when comparing model types, statistical methods generally showed higher consistency between runs than Neural Network-based methods. The exceptions here being the statistical models PCA and IForest (ranking 2nd and 5th), and the Neural Network-based model AutoEncoder (ranking 14th). HBOS was the only model to have its largest difference occur during a dataset with multiple hundred anomalies. Given the small absolute difference (0.00000291), this seems insignificant. Most prone to differences was the dataset \#175, appearing 5 times as the dataset with the highest absolute difference, followed by \#156 with 4 appearances. 
\begin{table}[htbp]
    \centering
    \label{tab:vus_pr_diff_summary_multi}
    \begin{tabular}{lrrrrrrr}
        \toprule
        \textbf{Model} & \textbf{\makecell[r]{Avg. Abs.\\Diff.}} & \textbf{\makecell[r]{Max. Abs.\\Diff.}} & \textbf{\makecell[r]{File\\ID}} & \textbf{\makecell[r]{Num. of\\Anom.}} & \textbf{\makecell[r]{Datasets\\>5\% Diff.}} & \textbf{\makecell[r]{Datasets\\>25\% Diff.}} & \textbf{\makecell[r]{Datasets\\>50\% Diff.}} \\
        \midrule
        \rowcolor{myLightBlue} TranAD & 0.185 & 0.910 & 175 & 4 & 83 & 36 & 28 \\
        \rowcolor{lightgray} PCA & 0.155 & 0.855 & 156 & 1 & 91 & 44 & 17 \\
        \rowcolor{myLightBlue} OmniAnomaly & 0.148 & 0.910 & 175 & 4 & 110 & 36 & 11 \\
        \rowcolor{myLightBlue} USAD & 0.147 & 0.909 & 175 & 4 & 109 & 36 & 8 \\
        \rowcolor{lightgray} IForest & 0.122 & 0.942 & 156 & 1 & 85 & 29 & 12 \\
        \rowcolor{myLightBlue} CNN & 0.095 & 0.922 & 187 & 6 & 70 & 15 & 12 \\
        \rowcolor{myLightBlue} LSTMAD & 0.086 & 0.952 & 175 & 4 & 57 & 10 & 6 \\
        \rowcolor{myLightBlue} FITS & 0.065 & 0.479 & 007 & 1 & 64 & 11 & 0 \\
        \rowcolor{myLightBlue} AnomalyTrans & 0.036 & 0.614 & 175 & 4 & 33 & 5 & 1 \\
        \rowcolor{lightgray} CBLOF & 0.030 & 0.870 & 194 & 5 & 8 & 7 & 5 \\
        \rowcolor{lightgray} KMeansAD & 0.019 & 0.566 & 046 & 1 & 19 & 1 & 1 \\
        \rowcolor{lightgray} RobustPCA & 0.015 & 0.396 & 014 & 1 & 17 & 1 & 0 \\
        \rowcolor{lightgray} EIF & 0.015 & 0.095 & 008 & 2 & 1 & 0 & 0 \\
        \rowcolor{myLightBlue} AutoEncoder & 0.009 & 0.318 & 017 & 1 & 6 & 2 & 0 \\
        \rowcolor{lightgray} KNN & 0.008 & 0.504 & 156 & 1 & 8 & 1 & 1 \\
        \rowcolor{lightgray} MCD & 0.007 & 0.544 & 194 & 5 & 2 & 1 & 1 \\
        \rowcolor{lightgray} LOF & 0.004 & 0.340 & 156 & 1 & 2 & 1 & 0 \\
        \rowcolor{lightgray} OCSVM & 0.004 & 0.370 & 017 & 1 & 2 & 2 & 0 \\
        \rowcolor{lightgray} HBOS & 1.71e-08 & 2.91e-06 & 123 & 463 & 0 & 0 & 0 \\
        \rowcolor{lightgray} COPOD & 9.61e-18 & 2.22e-16 & 008 & 2 & 0 & 0 & 0 \\
        \bottomrule
    \end{tabular}
    \par
    \vspace{1em}
    \noindent 
    \begin{tabular}{@{} p{1em} l @{}} 
        \rowcolor{lightgray} \strut & Statistical Methods \\
        \rowcolor{myLightBlue} \strut & Neural Network-based Method \\
    \end{tabular}
    \caption{\label{tab:Table 3} Summary of absolute differences in VUS-PR for multivariate datasets.}
\end{table}

\subsubsection{Benchmarking Conclusion}

Across both univariate and multivariate anomaly detection models, the observed trends regarding benchmark evaluation consistency are largely parallel. While most models displayed relatively small average differences between the two evaluations, a significant portion exhibited substantial discrepancies on individual datasets, particularly those with low anomaly counts where even minor variations in anomaly detection profoundly impacted scores. A consistent finding was that statistical methods generally demonstrated higher consistency than Neural Network-based models. However, notable exceptions exist, such as IForest often showing surprising inconsistency despite its configuration, and conversely, zero-shot models like Moment achieving remarkable stability due to their pre-trained, less randomizable nature. Given that most of the highest absolute differences can be found on recurring datasets, it is reasonable to assume that the specific type of anomalies and data sequences significantly influences the consistency between benchmarking runs. Furthermore, since a large portion of datasets with equally low anomaly counts are not present on this list, it can be ruled out that the higher differences between runs are entirely attributable solely to their low anomaly count. \par
However, it's important to stress that these observations - for univariate and multivariate models alike - are based on a very limited sample size of just two runs (the original authors' and my own). Therefore, while these results are noteworthy, this comparison doesn't provide any conclusive results regarding the general reproducibility of these models without further, more extensive investigation.

\section{Dataset Creation}
A common hurdle cited among Time Series Anomaly Detection researchers is the lack of high quality, accurately labeled data \parencites[p.~2]{liu2024elephant}[p.~1-6]{wu2021current}[p.~1]{paparrizos2022tsb}. Out of the existing datasets that fulfill the desired criteria, most are either entirely synthetic or contain artificially inserted anomalies that attempt to mimic real world scenarios based on statistical assumptions. To contribute to the current state of research, I will be providing 4 real, high quality, univariate datasets with accurate labels. \par

\subsection{Dataset Setup}
The provided datasets contain information about RAM utilization during matrix multiplication. While the first 3 datasets each include 1 unique type of anomaly, the fourth dataset combines all 3 anomalies. \par
\textbf{Baseline Load:} All four datasets have the same baseline load for their default state. A script performs continuous matrix multiplications, randomly selecting a matrix of size 3000, 3500, 4500, or 5000 and performing 3 multiplications of the selected size. This is followed by a sleep period of 4 seconds before another set of matrix multiplication resumes. Such a sequence of multiplications followed by a sleep time will be defined as an "baseline instance" going forward. Each dataset will first run for a predetermined amount of time without any anomalies, which can be used as the training split for semi-supervised models.

\begin{figure}[H] % htbp are common placement options (here, top, bottom, page)
    \centering % Centers the content (image and caption) horizontally
    \includegraphics[width=\textwidth]{baseline_sequence}
    \caption{Baseline Load}
    \label{fig:baseline} % Good practice for referencing the figure later
\end{figure}

\textbf{Dataset 1 - Large Matrix Injection:} For the first dataset, a particularily large matrix multiplication is performed as the anomaly. The generation starts with a large matrix multiplication to prepare the memory and avoid any changes in idle RAM utilization after the first anomaly injection. 
\begin{itemize}
    \item \textit{Baseload} -- Matrix Size: [3000, 3500, 4500, 5000] | Number of Multiplications: 3 | Sleep Time: 4 sec
    \item \textit{Runtime} -- Total: 90 min | Anomaly-free: 20 min 
    \item \textit{Preparation Matrix} -- Matrix Size: 6000 | Multiplications: 4
    \item \textit{Anomaly} -- Type: Large Matrix Injection | Matrix Size: 6000 | Multiplications: 4 | Frequency: 1/50 baseload instances
\end{itemize}

\begin{figure}[H] % htbp are common placement options (here, top, bottom, page)
    \centering % Centers the content (image and caption) horizontally
    \includegraphics[width=\textwidth]{anomaly_sequence_10}
    \caption{Large Matrix Injection}
    \label{fig:anomaly_10} % Good practice for referencing the figure later
\end{figure}


\textbf{Dataset 2 - Sleep Time Injection:} For the second dataset, an additional sleep time is performed instead of a matrix multiplication. This results in a sequence consisting of: regular sleep time -> anomaly sleep time -> regular sleep time. Given that the anomaly does not require more memory than the baselaod calculations, no preparation is requried at the start of the dataset generation.
\begin{itemize}
    \item \textit{Baseload} -- Matrix Size: [3000, 3500, 4500, 5000] | Number of Multiplications: 3 | Sleep Time: 4 sec
    \item \textit{Runtime} -- Total: 90 min | Anomaly-free: 20 min 
    \item \textit{Anomaly} -- Type: Sleep Time Injection | Sleep Time Duration: 2 sec | Frequency: 1/50 baseload instances
\end{itemize}

\begin{figure}[H] % htbp are common placement options (here, top, bottom, page)
    \centering % Centers the content (image and caption) horizontally
    \includegraphics[width=\textwidth]{anomaly_sequence_11}
    \caption{Sleep Time Injection}
    \label{fig:anomaly_11} % Good practice for referencing the figure later
\end{figure}

\textbf{Dataset 3 - Medium Matrix Injection:} For the third dataset, a matrix multiplication is performed as the anomaly. The size of the matrix falls between the already existing matrices in the baselaod calculations. Given that the anomaly does not require more memory than the baselaod calculations, no preparation is requried at the start of the dataset generation.
\begin{itemize}
    \item \textit{Baseload} -- Matrix Size: [3000, 3500, 4500, 5000] | Number of Multiplications: 3 | Sleep Time: 4 sec
    \item \textit{Runtime} -- Total: 90 min | Anomaly-free: 20 min 
    \item \textit{Preparation Matrix} -- Matrix Size: 4000 | Multiplications: 3
    \item \textit{Anomaly} -- Type: Medium Matrix Injection | Matrix Size: 4000 | Multiplications: 3 | Frequency: 1/50 baseload instances
\end{itemize}

\begin{figure}[H] % htbp are common placement options (here, top, bottom, page)
    \centering % Centers the content (image and caption) horizontally
    \includegraphics[width=\textwidth]{anomaly_sequence_12}
    \caption{Medium Matrix Injection}
    \label{fig:anomaly_12} % Good practice for referencing the figure later
\end{figure}

\textbf{Dataset 4 - Combined Anomaly Injection:} For the fourth dataset, all three anomalies are injected. The generation starts with a large matrix multiplication to prepare the memory and avoid any changes in idle RAM utilization after the first anomaly injection. The same combined anomaly frequency is maintained as in the previous three datasets. The total length of the dataset is adjusted upwards. 
\begin{itemize}
    \item \textit{Baseload} -- Matrix Size: [3000, 3500, 4500, 5000] | Number of Multiplications: 3 | Sleep Time: 4 sec
    \item \textit{Runtime} -- Total: 160 min | Anomaly-free: 25 min 
    \item \textit{Preparation Matrix} -- Matrix Size: 6000 | Multiplications: 4
    \item \textit{Anomaly 1} -- Type: Large Matrix Injection | Matrix Size: 6000 | Multiplications: 4
    \item \textit{Anomaly 2} -- Type: Sleep Time Injection | Sleep Time Duration: 2 sec
    \item \textit{Anomaly 3} -- Type: Medium Matrix Injection | Matrix Size: 4000 | Multiplications: 3 
    \item \textit{Combined Anomaly Frequency} 3
\end{itemize}

\subsection{Logging}

With real data, the biggest challenge is not to find extensive datasets, it's to find datasets with accurate labels. Without those, (semi-)supervised models cannot function and no models can be properly evaluated. Given this challenge, creating an accurate logging mechanism that will correctly label datapoints as either normal or anomalous was at the core of this entire seminar paper. \par
The logging system employs a dual approach that provides a comprehensive view of each action executed and accurate monitoring of Resident Set Size (RSS) memory usage. It captures both discrete events and continuous system state. At its center is a centralized, thread-safe function called log\_entry which appends timestamped records to a CSV file. It utilizes a global threading.Lock to serialize file access, preventing race conditions and ensuring that multiple logging instances can write log entries without corruption. \par
The first part is event-driven and activates whenever the script is started, an instance of matrix multiplication is started, or an instance of matrix multiplication concludes. These event logs provide a clear overview of all operations. \\
The second part of my logging system (continuous\_logger\_thread) continuously samples current RSS memory usage while running in parallel to the rest of the working script. It monitors the current state of operations in 0.1 second intervals. If it detects the state changing from 'idle' to 'working', it waits for 0.3 seconds before recording RSS memory usage. During testing, I encountered the issue of a matrix multiplication having technically started but no changes in actual hardware utilization has yet occurred. The 0.3 seconds delay ensures that the first datapoint after a workload has started is correctly labeled as "working". This is particularly important for when an anomaly has been injected. Similarly, when it detects a state change from 'working' to 'idle', the logger waits 0.1 seconds before recording RSS memory usage. This exists to prevent situations where the matrix multiplication has already ended but the memory was not yet released. While none of these state changes occur, the logger takes a snapshot every 1.0 seconds. The 0.3 seconds and 0.1 seconds were each originally selected during extensive testing on a dataset that was supposed to track CPU usage. When used on memory, it continued to function as intended and was therefore kept.

\subsection{Hardware Setup}

The simulations are ran on the previously described system that was used for the replication of the TSB-AD benchmark. Since Windows is notorious for unannounced and unwanted background activities that can often neither be reliably prevented, nor accounted for, a VMWare Workstation Virtual Machine was used. The specific setup and resource allocations were as follows:

\begin{itemize}
    \item 4 Processor Cores (2 Processors with 2 Cores each)
    \item 8GB DDR5 RAM
    \item OS: Linux Ubuntu 24.04.2 LTS 64-bit
    \item Automatic Updates Disabled
    \item Networking Disabled
\end{itemize}

\subsection{Statistical Overview}


\begin{table}[htbp]
    \centering
    \label{tab:dataset1_4_stat_overview}
    \begin{tabular}{lrrrrrrr}
        \toprule
        \textbf{} & \textbf{\makecell[r]{Dataset 1}} & \textbf{\makecell[r]{Dataset 2}} & \textbf{\makecell[r]{Dataset 3}} & \textbf{\makecell[r]{Dataset 4}}\\
        \midrule
        Anomaly Type & Large Matrix & Added Sleep & Medium Matrix & Combined \\
        Length & 5308 & 5320 & 5319 & 9452 \\
        Total Anomaly Sequences & 8 & 11 & 11 & 21 \\
        Total Anomalous Points & 91 & 110 & 40 & 194 \\
        Average Anomaly Length & 11.38 & 10.0 & 3.64 & 9.24 \\
        Longest Anomaly & 13 & 10 & 4 & 16 \\
        Shortest Anomaly & 10 & 10 & 3 & 4 \\
        Anomaly Ratio (Point based) & 0.0171 & 0.0207 & 0.0075 & 0.0205 \\
        Anomaly Ratio (Sequence based) & 0.0015 & 0.0021 & 0.0021 & 0.0022 \\
        RC Score & 473.079 & 505.050 & 395.150 & 597.475 \\
        \bottomrule
    \end{tabular}
    \par
    \vspace{1em}
    \textit{Anomaly Ratio (Point based): Total Anomalous Points divided by Length} \\
    \textit{Anomaly Ratio (Sequence based): Total Anomalous Sequences divided by Length}
    \noindent
    \caption{\label{tab:Table 4} Summary of absolute differences in VUS-PR for univariate datasets.}
\end{table}

Most datasets used for the TSB-AD benchmark are between 1000 and 50000 datapoints in length. Based on this, I decided to keep Datasets 1-3 at around 5000 datapoints. This ensures a sufficiently large sample size, while keeping it small enough for me to manually go through the entire set and check for potential problems or mislabeled datapoints. Since Dataset 4 contains 3 separate anomaly types and I intended to maintain a consistent anomaly ratio between all sets, I increased its size. Otherwise, when maintaining the same anomaly ratio, it would be possible for some anomaly type to not occur at all. \cite[pp.~8]{SchmidlEtAl2022Anomaly} define an acceptable anomaly ratio to be one below 10\%, as anything above it would be better considered to be a classification problem. The median anomaly ratio in the TSB-AD dataset is 4.85\% and \cite[p.~4]{wu2021current} consider the adequate number of anomalies to be exactly 1. Given his range of opinions, I settled on a range of between 0.7\% and 2.2\% for my datasets. The differences between datasets can be explained by the average length of an anomalous sequence depending on anomaly type. The actual anomaly ratio based on anomalous sequences to total sequences is kept steady between datasets. Maintaining the sequence ratio ensures each dataset presents a consistent number of anomalous events to detect, which is more important for a fair comparison than the point ratio that can be skewed by the varying lengths of those events. \par
For the calculations of the difficulty measures, I used Relative Contrast (RC), as described by \cite[pp.~1704]{paparrizos2022tsb}. Since the authors did not specify their preferred Shape Based Distance (SBD) metric, I applied the commonly used Dynamic Time Warping (DTW), using the cdist\_dtw implementation provided by the tslearn library \parencites{DTW}. \cite[pp.~1704]{paparrizos2022tsb} do not clearly state how to decide on a sliding window for the comparison of individual sequences and only state to "split the time series by its period" \parencite[p.~1704]{paparrizos2022tsb}. I interpret the term "period" to mean a repeating pattern, as is common in time series analysis. With all 4 of my created datasets, a repeating period can be defined as a sequence of matrix multiplications followed by an idle state, which tends to be between 8 and 12 datapoints long. With that in mind, I selected the value 10 for my sliding window. Since I neither know the exact RC implementation of \cite{paparrizos2022tsb}, nor their used SBD metric, it is impossible to compare the here produced values with those provided in the TSB-UAD paper. Therefore, a comparison can only be made between my own datasets. A low RC score should indicate that its difficult to effectively cluster the points in a dataset, making anomaly detection theoretically more challenging. Based on this assumption, Dataset 4 should provide the easiest to detect anomalies. As will be discussed in the next section, this does not hold up.

\subsection{Benchmarking}

\begin{table}[htbp]
    \centering
    % Define the number of columns: 1 for labels (l) and 4 for data (r)
    \begin{tabular}{lrrrr}
        \toprule
        \textbf{} & \textbf{\makecell[r]{Dataset 1}} & \textbf{\makecell[r]{Dataset 2}} & \textbf{\makecell[r]{Dataset 3}} & \textbf{\makecell[r]{Dataset 4}}\\
        \midrule
        % First row with anomaly types, but no main label
        Anomaly Type & Large Matrix & Added Sleep & Medium Matrix & Combined \\
        \midrule % A line to separate the main info from the sections
        
        % VUS-PR Section
        \multicolumn{5}{l}{\textbf{VUS-PR}} \\
        \hspace{1em}Best Model & Sub-PCA & KShapeAD & LOF & OmniAnomaly \\
        \hspace{1em}Best Score & 0.989 & 0.562 & 0.062 & 0.394 \\
        \hspace{1em}Avg. Score & 0.376 & 0.096 & 0.018 & 0.084 \\
        \hspace{1em}Avg. Score (St. Mod.) & 0.253 & 0.144 & 0.024 & 0.062 \\
        \hspace{1em}Avg. Score (NN Mod.) & 0.472 & 0.059 & 0.014 & 0.108 \\
        
        \cmidrule(r){1-5} % A partial rule to separate the two sections
        
        % VUS-ROC Section (with placeholder data)
        \multicolumn{5}{l}{\textbf{VUS-ROC}} \\
        \hspace{1em}Best Model & Sub-PCA & SAND & LOF & OmniAnomaly \\
        \hspace{1em}Best Score & 0.999 & 0.949 & 0.926 & 0.878 \\
        \hspace{1em}Avg. Score & 0.832 & 0.586 & 0.554 & 0.615 \\
        \hspace{1em}Avg. Score (St. Mod.) & 0.887 & 0.572 & 0.634 & 0.613 \\
        \hspace{1em}Avg. Score (NN Mod.) & 0.767 & 0.604 & 0.506 & 0.659 \\
        
        \bottomrule
    \end{tabular}
    \caption{VUS-PR and VUS-ROC: Summary of benchmarking results for datasets 1-4.}
    \label{tab:VUS-PR-ROC_benchmarking_overview}
\end{table}

Table 5 provides a summary of results when evaluating datasets 1-4 with the TSB-AD benchmark.
Unsurprisingly, dataset 1, which essentially just has a larger peak load as its anomalies, had the best results. Sub-PCA managed to achieve almost perfect scores for both VUS-PR (0.989) and VUS-ROC (0.999), highlighting its ability to detect the existing anomalies precisely and with great sensitivity. Most other models, statistical and neural network-based alike, also performed decently well with average scores (VUS-ROC) of 0.887 and 0.767 respectively.
When looking at VUS-PR score averages, most detection models did not reach a score of above 0.500, with neural-network based models surprisingly outperforming statistical models. Since all anomalies are sequence-based and not point-based, one would expect the opposite \parencites[p.~10]{liu2024elephant}[p.~1706]{paparrizos2022tsb}.
For dataset 2, which introduces anomalies by extending the idle time, the results are more varied. The best VUS-ROC score was a high 0.949 achieved by SAND, indicating a strong ability to detect these anomalies. However, the best VUS-PR score was only 0.562 from KShapeAD, showing that while models can find the anomalies, they struggle with precision. The average VUS-ROC scores were middling, with neural network models (0.604) slightly outperforming statistical ones (0.572). In contrast, the average VUS-PR scores were poor across the board, with statistical models (0.144) performing better than neural network models (0.059).
Dataset 3, featuring the 'Medium Matrix' anomalies, was clearly the most difficult for the benchmarked models. This is especially evident in the VUS-PR scores, which were exceptionally low. The LOF model performed best on both metrics, yet it highlights a significant performance gap: it achieved a strong VUS-ROC score of 0.926 but a dismal VUS-PR score of only 0.062. On average, VUS-ROC scores were moderate, with statistical models (0.634) performing noticeably better than neural network models (0.506). This trend was even more pronounced for the VUS-PR scores, where the average for all models was a near-zero 0.018, confirming the difficulty of this dataset. It is also the only dataset where the best VUS-PR and VUS-ROC score where not achieved by the same model.
Dataset 4, which aggregates all anomaly types, was intended to be the most challenging. The neural network-based OmniAnomaly model achieved the best scores for both VUS-PR (0.394) and VUS-ROC (0.878). While the VUS-ROC score demonstrates good overall sensitivity, the sub-0.400 VUS-PR score again points to a significant challenge in maintaining precision across varied anomaly types. The average VUS-ROC scores were the highest outside of dataset 1, with neural network models (0.659) holding a slight edge over statistical models (0.613).
Other than the results for dataset 1, these scores are unexpectedly bad. While an average VUS-ROC score of around 0.500 for datasets 2 and 3 and could potentially be deemed acceptable, when combined with the horrendous VUS-PR scores, it becomes questionable whether most of the models provided any value at all. This applies even more so to dataset 3, where not a single model was able to achieve a usable level of precision. Given that the anomalies were correctly labeled, this begs the obvious question: Were the anomalies simply too difficult to detect? In order to answer this, I have applied a modified process introduced by \cite{wu2021current}, who attempted to detect anomalies with only a single line of code in order to determine whether they are trivial. Since the goal here is not to look for triviality but instead an acceptable level of difficulty, I have attempted to write a simple script, made up of only basic if-statements, which can, with high accuracy and precision, detect all existing anomalies.
\textit{Dataset 2 Pseudo Code:} \par
\begin{lstlisting}
    For i from 0 to len(data)-1:
        If data[i] < 60:
            Count consecutive values < 60
            If count > 7:
                Mark those indices as anomalies
    Return anomaly labels
\end{lstlisting}
\par
When appling this simple algorithm, it returns 110 True Positives, 0 False Positives, 0 False Negatives, and therfore a precision and sensitivity of 1 are achieved.\par
\textit{Dataset 3 Pseudo Code:} \par
\begin{lstlisting}
    For i from 0 to len(data)-1:
    If data[i] in (530, 700):
        Count consecutive values in (530, 700)
        If count == 3 or 4:
            Mark those indices as anomalies
    Return anomaly labels
\end{lstlisting}
\par
Similarly here, the algorithm returns 40 True Positives, 0 False Positives, 0 False Negatives, and therefore a precision and sensitivity of 1. \par
This is obviously not a perfect test, as it requires me to have prior knowledge about the nature of the anomaly. Despite that, if an algorithm of such simplicity can detect anomalies with complete precision and sensitivity, I will argue, that any other sufficiently advanced detection method should be able to at least reach a VUS-PR score of above 0.1 to be deemed functional.\par
The anomaly ratios (<3\%) are firmly within the established consensus for what an acceptable ratio is, the datapoints are accurately labeled with the help of an automated logger and a manual inspection, the anomalies represent real changes in workload, and the anomalies can be easily detected using a few lines of code. Based on these aspects I am willing to state that the results do not represent poor quality of the datasets but instead a blindspot in the current set of detection models included by the TSB-AD benchmark. \par
Since Dataset 4 is made up two-thirds of the anomalies established in Datasets 2 and 3, it is only natural that the average scores are also rather unimpressive and lie somewhere between those of Aataset 1 and Dataset 4. The inclusion of multiple anomaly types seems to have slightly degraded peak VUS-ROC performance compared to all other datasets. This is most likely the result of highly specialized models that are good at detecting certain anomalies (e.g. peak values), struggling more with other types of anomalies. A neural network-based model performing best in both metrics is another notable occurrence.

\section{Conclusion}

[Placeholder]: Datasets suprisingly challenging. Exposes a potential blindspot of current models. Since they struggle with medium and bottom values, blindspot appears to be anomalies that can only be discovered in the context of the whole dataset or at least a large portion. The anomalies are not possible to see when only compared with the local trend and need an understanding of the bigger picture.

\section{Discussion}

\section{Limitations}






%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Literaturverzeichnis wird 
%% automatisch eingefügt
%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\clearpage
\lhead{}
\printbibliography
\addcontentsline{toc}{section}{\bibname}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Anhang (optional) 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\clearpage
\appendix
\section{Anhang A}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Eidesstattliche Erklärung
%% muss angepasst werden 
%% in Erklaerung.tex
%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\input{Erklaerung.tex}

\end{document}
