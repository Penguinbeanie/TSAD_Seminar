\documentclass[12pt,oneside]{article}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%   Zusaetzliche Pakete  %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\usepackage{enumerate}  
\usepackage{fancyhdr}
\usepackage{a4wide}
\usepackage{graphicx}
\usepackage{palatino}
\usepackage{multirow}
\usepackage{booktabs}
\usepackage{titlesec}
\usepackage{enumitem}% http://ctan.org/pkg/enumitem

%folgende Zeile auskommentieren für englische Arbeiten
%\usepackage[ngerman]{babel}
%folgende Zeile auskommentieren für deutsche Arbeiten
\usepackage[english]{babel}

\usepackage[T1]{fontenc}
\usepackage{paratype}\renewcommand{\familydefault}{\sfdefault}
\usepackage[utf8]{inputenc}
\usepackage[bookmarks]{hyperref}
\usepackage[justification=centering]{caption}
\usepackage[style=authoryear,natbib=true,backend=biber,maxbibnames=20]{biblatex}
\usepackage{csquotes}
\usepackage{booktabs}
\usepackage{makecell}
\bibliography{literatur}

\setlength{\parindent}{0em} 
\setlist[itemize]{noitemsep, topsep=0pt}
\setlist[enumerate]{noitemsep, topsep=0pt}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Definition der Kopfzeile %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\pagestyle{fancy}
\fancyhf{}
\cfoot{\thepage}
\setlength{\headheight}{16pt}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%  Definition des Deckblattes und der Titelseite  %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\newcommand{\JMUTitle}[9]{

  \thispagestyle{empty}
  \vspace*{\stretch{1}}
  {\parindent0cm
  \rule{\linewidth}{.7ex}
  }
  \begin{flushright}
    \sffamily\bfseries\Huge
    #1\\
    \vspace*{\stretch{1}}
    \sffamily\bfseries\large
    #2\\
    \vspace*{\stretch{1}}
    \sffamily\bfseries\small
    #3
  \end{flushright}
  \rule{\linewidth}{.7ex}

  \vspace*{\stretch{1}}
  \begin{center}
    \includegraphics[width=2in]{siegel} \\
    \vspace*{\stretch{1}}
    \Large #5 \\

    \vspace*{\stretch{2}}
   \large Lehrstuhl f\"{u}r Wirtschaftsinformatik und Business Analytics\\
    \large Universität Würzburg\\
    \vspace*{\stretch{1}}
    \large Betreuer:  #8 \\[1mm]
    \large Assistent:  #9 \\[1mm]
    \vspace*{\stretch{1}}
    \large #6, den #7
  \end{center}
}

\titlespacing*{\section}
{0pt}{3.5ex plus 1ex minus .2ex}{.2ex plus .2ex}
\titlespacing*{\subsection}
{0pt}{1.5ex plus 1ex minus .2ex}{.2ex plus .2ex}
\titlespacing*{\subsubsection}
{0pt}{1.5ex plus 1ex minus .2ex}{.2ex plus .2ex}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%  Beginn des Dokuments  %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}
  \JMUTitle
      {Time Series Anomaly Detection Benchmarking}        % Titel der Arbeit
      {Philip Spaier}                        % Vor- und Nachname des Autors
      {3110375}
      
      {Seminararbeit } % Art der Arbeit
      {W"urzburg}                           % Ort der Erstellung
      {05.04.2025}                          % Tag der Abgabe
      {Prof. Dr. Gunther Gust}           % Name des Erstgutachters
      {Viet Nguyen} % Name des/der betreuenden Assistent/Assistentin
      
  \clearpage

\lhead{}
\pagenumbering{Roman} 
    \setcounter{page}{1}

\tableofcontents
\clearpage

\addcontentsline{toc}{section}{\listfigurename}
\listoffigures

\addcontentsline{toc}{section}{\listtablename}
\listoftables
\clearpage

\setlength{\parskip}{0.5em} 


%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%  Kurzzusammenfassung   %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\lhead{Abstract}
\section*{Abstract}

Eine Kurzzusammenfassung der Vorgehensweise und der wesentlichen Ergebnisse.

Allgemeine Merkmale
\begin{itemize}
    \item Objektivität: Es soll sich jeder persönlichen Wertung enthalten.
    \item Kürze: Es soll so kurz wie möglich sein.
    \item Verständlichkeit: Es weist eine klare, nachvollziehbare Sprache und Struktur auf.
    \item Vollständigkeit: Alle wesentlichen Sachverhalte sollen enthalten sein.
    \item Genauigkeit: Es soll genau die Inhalte und die Meinung der Originalarbeit wiedergeben.
\end{itemize}{}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%  Einstellungen  %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\clearpage
\pagenumbering{arabic}  
    \setcounter{page}{1}
\lhead{\nouppercase{\leftmark}}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%  Hauptteil  %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\section{Literature Review} \label{einleitung}
Time Series Anomaly Detection (TSAD), as a subcategory of the broader field of Anomaly Detection, has seen increased attention since the start of the twenty first century. With the internet having established itself as a persistent and omnipresent force in every imaginable aspect of human life, time series data can be found in abundance. Modern developments in Internet-of-Things (IoT) applications, the digitization of financial data, and a massive rise in the consumption of streaming services have contributed to an exponential growth of time series data [source needed]. This in turn has made the manual search of potential anomalies in many fields completely infeasible, leading to an increased demand for automated anomaly detection methods. While there is a continuously growing repertoire of such automated detection methods, the lack of a generally accepted and reliable benchmark makes not just further developments but also the selection of appropriate models difficult. In the following sections of this literature review, I will provide the reader with a better understanding of context independent Time Series Anomaly Detection, the most commonly applied methods, and the current state of benchmarking.


\subsection{Time Series Data and Anomaly Detection Definition}
Time Series Data, as used in the rest of this thesis, shall be defined as follows: a sequence of data or observations, typically indexed by or associated with specific timestamps, collected in chronological order over a period of time.
For the purpose of analysis, continuous signals must be converted into individual data points. Each datapoint can either represent a binary state (1 or 0), be a numerical value measured on a ratio scale (eg. number of occurrences), or a numerical value measured on an interval scale (eg. temperature on a Celsius scale).
A time series with a dimensionality of one (only a single feature) will be referred to as "univariate", while a time series with higher dimensionality (multiple features) will be referred to as "multivariate". \\
An anomaly will be defined as follows: an abnormal, rarely occurring data point or sequence, that has to be be detectable with exclusively context independent methods. Individual anomalous data points will be referred to as "point based" anomalies. Multiple consecutive anomalous points, each of which might be unremarkable on their own, while displaying unusual behavior as a sequence,  will be referred to as "sequence based" or "collective" anomalies \parencites[p.~3]{liu2024elephant}[p.~8]{chalapathy2019deeplearninganomalydetection}. A separate category of anomalies would be context dependent ones. Those are data points or sequences, possibly indistinguishable from normal ones if analyzed without context, but if combined with additional information about the field or time series, are considered anomalous \citep[pp.~7-8]{chalapathy2019deeplearninganomalydetection}. Context dependent anomalies will not be topic of the research presented here. \\
Given those definitions, Time Series Anomaly Detection is therefore the task of correctly and autonomously identifying anomalies within a given time series.

Insert Images of point vs sequence.

\subsection{Relevant Fields}

The following is an overview of fields relying on Time Series Anomaly Detection. It is a non-exhaustive list, simply highlighting some of the most prominent use cases to provide context. \

\textbf{Illicit Activity and Fraud Detection:} With the global financial system relying primarily on digital transactions, it has become crucial to detect fraudulent activities as quickly and accurately as possible. A particularly obvious example is credit card fraud, creating an estimated yearly loss in the billions of dollar \parencites[p.~2]{FinFraud}. Companies like Visa and Mastercard put great emphasis on being able to detect anomalous transactions in real time to then analyses them and prevent potential harm to their customers \citep{VisaFr}. While credit card fraud is a prominent application, the scope of financial anomaly detection extends significantly further, playing a critical role in the operations of stock exchanges, brokerage firms, and banks. These institutions leverage anomaly detection techniques to identify various illicit activities, ensure market integrity, manage operational risks, and comply with stringent regulatory requirements \citep{DBSurv}. \

\textbf{Healthcare:} Healthcare critically relies on analyzing physiological signals, such as those captured by the electrocardiogram (ECG), which provides vital time series data reflecting the heart's electrical activity. While historically, ECG analysis has focused on identifying established patterns of known heart diseases, this approach often fails to detect rare or atypical anomalies that do not fit predefined categories, potentially missing critical conditions. To address this issue, Time Series Anomaly Detection has been introduced for the purpose of detecting such rare anomalies that would go unnoticed by conventional pattern classification \parencites[p.~1-2]{jiang2024anomalydetectionelectrocardiogramsadvancing}.

\textbf{Website Traffic:} A common threat faced by web-services are so called Denial or Service (DoS) and Distributed Denial of Service (DDoS) attacks. These include hitting a webserver with so many requests that the systems becomes inoperational and can no longer service legitimate users \citep{BSI1}. A significant challenge in detecting these attacks is that the malicious traffic can often mimic normal network traffic, making it difficult for traditional packet-based intrusion detection systems or statistical methods reliant on fixed thresholds to accurately identify attacks, especially when they are hidden within legitimate flows. Time series analysis allows systems to observe and distinguish the instant changes in network traffic that indicate an attack, even when individual packets or simple statistics are insufficient. Time series anomaly detection provides a means to autonomously identify and localize potentially harmful deviations within the network traffic and thereby ensure the availability and reliability of services \parencites[pp.~1-2]{FOULADI2020102587}. \par
The list extends far beyond the fields named above. Time Series Anomaly Detection can be also found in astronomy \citep{astro}, earth sciences, manufacturing \parencites[p.~1]{Zamanzadeh_Darban_2024}, cybersecurity, and law enforcement \parencites[p.~1]{boniol2024divetimeseriesanomalydetection}.


\subsection{Detection Methods}
Detection methods, in common descriptions and within the scientific literature, are often grouped or distinguished by a variety of aspects. This categorization can sometimes lack a consistent taxonomy. To provide a clearer framework, I will now systematically explain and categorize these methods through three key perspectives:
\begin{itemize}
    \item Degree of supervision
    \item Architecture
    \item Technique
\end{itemize}

\subsubsection{Degree of Supervision} \textit{Unsupervised models} operate on data without any explicit labels distinguishing normal from anomalous instances. While they don't require pre-labeled data, they typically do require a training or fitting phase. During this phase, the model learns the inherent structure, patterns, distributions, or densities from the unlabeled dataset.\par \textit{Semi-supervised models} are trained exclusively on data that is known or assumed to be 'normal.' They do not require labeled anomalies for training. The model learns a precise representation or boundary of this normal behavior. During deployment, any new data instance that significantly deviates from this learned model of normalcy is flagged as an anomaly. \par \textit{Supervised models} require a dataset where both normal and anomalous instances are explicitly labeled beforehand. The model is then trained to learn the distinguishing features or decision boundaries that separate these classes, effectively treating anomaly detection as a (often highly imbalanced) classification problem. \parencites[pp.~5-6]{boniol2024divetimeseriesanomalydetection}[p.~3]{liu2024elephant}[p.~3-4]{SchmidlEtAl2022Anomaly}.

\subsubsection{Architecture} \textit{Statistical models} identify anomalies by relying on statistical assumptions to detect deviations from expected data distributions. They often involve fitting a distribution model to the data and measuring abnormality based on probabilities or distances from the calculated distribution. Statistical models often require a threshold to be set beforehand\parencites[p.~6-7]{liu2024elephant}[p.~1]{FOULADI2020102587}. \par \textit{Neural Network based models} are a collection of distributed, adaptive, non-linear processing units with adjustable weights \parencites[p.~427]{GURESEN2011426}. They rely on a training dataset and are often semi-supervised. Deep neural networks, a subcategory of neural networks, model spacial and temporal dependencies \parencites[p.~6-7]{liu2024elephant}[6]{Zamanzadeh_Darban_2024}.\par \textit{Foundational Models} utilize transfer learning, using knowledge from a different class of tasks and then applying it on the target task. These models are pre-trained and are then being fine-tuned \parencites[p.~4]{bommasani2022opportunitiesrisksfoundationmodels}. In the context of TSAD, those models are GPT models fine tuned on time series data, general purpose time series models, or originally time series classification models now used for anomaly detection \parencites[p.~7]{liu2024elephant}.

\subsubsection{Technique} \textit{Distance based models} work on the idea that anomalous points or sequences will further away when using a distance measurement. They can be either be compared to their nearest neighbor, all other points/subsequences, or cluster centers \parencites[p.~6]{SchmidlEtAl2022Anomaly}. Such distances are calculated in various ways depending on the model and implementation, with the most common definitions being the Euclidean distance or the Z-normalized Euclidean distance. Distance based models use only the x- and y-axis data, with no labels being required \parencites[p.~8]{boniol2024divetimeseriesanomalydetection}. \par \textit{Forecasting models} learn the normal patters of a time series and, often using a sliding context window, forecast the next datapoint in the series. The forecasted and actual data points are then compared, with the difference being used for an anomaly score. Given a high enough anomaly score, a point is considered an anomaly. Such models are usually semi-supervised \parencites[p.~4-5]{SchmidlEtAl2022Anomaly}. \par \textit{Isolation Tree Models} use ensembles of random trees, selecting random features and splits, to separate points or sequences from each other. It operates on the idea that anomalies require fewer steps to be separated from the rest of the data than normal points/sequences. For each point/sequence, the distance from the root is calculated. The shorter a distance is, the more likely is a point/sequence to be an anomaly. These models can be both unsupervised and supervised \parencites[p.~6-7]{SchmidlEtAl2022Anomaly} \par \textit{Distribution based models} estimate a distribution of the time series and then score individual points or sequences as anomalous or normal based on it. Anomalous points are expected to have a low probability. Alternatively to probabilities, the anomaly score can also be calculated using likelihoods or distances. These models are generally unsupervised or occasionally semi-supervised \parencites[p.~6]{SchmidlEtAl2022Anomaly}. \par \textit{Graph based models} methods turn time series data, or parts of it, into a graph structure. This graph represents the different types of patterns (subsequences) found in the data as nodes, and how these patterns follow each other over time as connections (edges) between the nodes. Anomalies are then determined based on usual structures or behaviors found in the graph \parencites[p.~23-24]{boniol2024divetimeseriesanomalydetection}. Graph based time series models can be further divided in multiple subcategories, including AutoEncoder- and GAN-based methods, as well as predictive graph models \parencite{ho2025graphanomalydetectiontime}. \par \textit{Reconstruction models} learn a time series' features and patterns by encoding normal data into a low dimensional space. Given a test dataset, they compress test data and reconstruct it using their model based on that low-dimensional space. Should a point or sequence of this reconstructed version deviate substantially from the actual data, then it is labeled as anomalous. These models are often considered semi-supervised because they typically use normal labeled data for training. However, models that do not rely on a training dataset and instead directly encode and reconstruct the test data also exist, operating in an unsupervised manner. \parencites[p.~5]{SchmidlEtAl2022Anomaly}. \par \textit{Encoder based models} operate similarly to reconstruction models. They compress a given time series into a low-dimensional representation, but instead of reconstructing it, they directly compare this compressed version to their model of normal time series. Anomalous points or sequences might have unusual encoded representations, and their deviations from the normal model are then used to calculate an anomaly score \parencites[p.~5-6]{SchmidlEtAl2022Anomaly}.

\subsection{Performance Metrics}

For the effective evaluation of a models performance, as defined by \cite{papaVus}, metrics have to fullfil the following criteria:
\begin{itemize}
    \item \textit{Robustness to Lag:} The evaluation measure should be insensitive to slight temporal shifts or lags in anomaly scores.
    \item \textit{Robustness to Noise:} The evaluation measure should be stable and unaffected by noise in the anomaly scores.
    \item \textit{Robustness to Anomaly Cardinality Ratio:} The evaluation measure's score should not be influenced by the proportion of anomalies in the data.
    \item \textit{High Separability between Accurate and Inaccurate Methods:} The measure must effectively distinguish between accurate and inaccurate detection methods.
    \item \textit{Consistency:} The measure should produce repeatable scores for similar data and consistently rank different methods.
\end{itemize}
Commonly applied performance measures for TSAD can generally be classified based on two characteristics: Point-wise or Range-wise, and Threshold-dependent or Threshold-independent.


\subsubsection{Point-wise or Range-wise}
\textit{Point-wise evaluation measures} look at each anomalous point independently, determining in a binary fashion whether a model classified them correctly as normal or anomalous \parencite[p.~7]{liu2024elephant}. These measures suffer from a variety of issues. Most crucially, they can unfairly penalize methods that detect only part of an anomalous range or whose detection peak doesn't perfectly align with the labeled range. Further more, they are sensitive to temporal lag. Should an anomalous data point be detected slightly before or after the actual anomaly occurs, a fully point-wise metric will score it with an unreasonably low score \parencites[p.~2778]{papaVus}.\\ \textit{Range-wise measures} look at anomalies not just from the perspective of individual points but take sequences into consideration. For anomalous sequences, their evaluation can involve determining how much the detected and the actual sequence overlap. Additionally, such measures may incorporate strategies like adequately handling lag (e.g., by considering an anomaly detected if it's within a specified range of an actual one, even if not at the exact spot) or including a cardinality factor to penalize models that incorrectly segment anomalies (such as detecting multiple short ones for a single large event, or vice-versa) \parencite[p.~7]{liu2024elephant}.
\subsubsection{Threshold-dependent or Threshold-independent}
\textit{Threshold-dependent measures} require a threshold to be set that determines whether an anomaly score classifies a value as anomalous or normal. This can be done based on statistical assumptions, or using dynamic algorithms that adjust to the data and results \parencites[p.~38-39]{boniol2024divetimeseriesanomalydetection}. Setting these thresholds automatically, however, is often difficult when working with large and diverse datasets, and the chosen thresholds can drastically change a metric's accuracy. Noise and the normal-to-anomalous ratio in a time series can be particularly problematic \parencites[p.~2777-2778]{papaVus}. \\
\textit{Threshold-independent measures} evaluate the performance of a time series anomaly detection method without needing a specific score cutoff to decide what constitutes an anomaly. Instead of relying on a fixed threshold, they assess how effectively the method's anomaly scores rank true anomalies higher than normal data points across the entire range of scores \parencites[p.~39-41]{boniol2024divetimeseriesanomalydetection}.

\subsubsection{Definition and Classifgication of Metrics}

The following defines the most commonly used metrics and classifies them into the above described categories \parencites[p.2776-2780]{papaVus}:
\begin{itemize}
    \item \textit{Precision / Range Precision:} number of correctly identified anomalies over all anomalies.
    \item \textit{Recall (TPR) / Range Recall:} number of correctly identified anomalies over all anomalies.
    \item \textit{F-Score / Range F-Score:} Harmonic Mean of Precision and Recall. 
    \item \textit{False Positive Rate (FPR):} number of points wrongly identified as anomalies over the total number of normal points.
    \item \textit{AUC-ROC:} area under the curve corresponding to TPR on the y-axis and FPR on the x-axis at all threshold levels.
    \item \textit{AUC-Precision:} area under the curve corresponding to the Recall on the x-axis and Precision on the y-axis at all threshold levels.
    \item \textit{VUS-ROC:} generating multiple ROC curves for a range of different buffer lengths. These stacked ROC curves form a 3D surface, and VUS-ROC is the volume beneath this surface.
    \item \textit{VUS-Precision:} generating multiple Precision-Recall curves for a range of different buffer lengths. These stacked PR curves form a 3D surface, and VUS-Precision (VUS-PR) is the volume beneath this surface.
\end{itemize}




\begin{table}[htbp]
  \centering
  \label{tab:evaluation_categories}

  \begin{tabular}{lcc}
    \toprule
    & Threshold-dependent & Threshold-independent \\
    \midrule
    Point-wise & \makecell[l]{Precision \\ Recall \\ False Positive Rate \\ F-Score} & \makecell[l]{AUC-ROC \\ AUC-Precision} \\
    \midrule
    Range-wise & \makecell[l]{Range Precision \\ Range Recall \\ Range F-Score} & \makecell[l]{VUS-ROC \\ VUS-Precision} \\
    \bottomrule
  \end{tabular}
  \caption{Evaluation Measures}

\end{table}

\textbf{}


\subsection{State of Benchmarking}

\section{Dataset Analysis}

\textbf{Das ist fett gedruckter Text}.

\textit{Das ist kursiver Text}.


Auflistungen sind oft hilfreich für die Strukturierung:
\begin{itemize}
    \item Erster Eintrag
    \item Zweiter Eintrag
\end{itemize}

Nummerierte Aufzählungen sind oft hilfreich für Reihenfolgen:
\begin{enumerate}
    \item Erster Eintrag
    \item Zweiter Eintrag
\end{enumerate}

\section{Replication of TSB-AD Benchmark Results}

\section{Dataset Creation}

\section{Conclusion}

\section{Zitieren und Referenzieren}

Beiträge in Fachzeitschriften wie \citet{clemen1989combining} oder Konferenzartikel wie \citet{he2017mask} werden auf diese Weise im Text zitiert. In anderen Fällen möchte man aber in Klammern zitieren \citep{clemen1989combining}, auch mit mehreren Autoren \citep{clemen1989combining,baumol1958warehouse,he2017mask}.

Bei Monographien muss eine Seitenzahl mit angegeben werden \citep[S. 28]{chollet2018deep}.

So wird eine Webquelle zitiert: \citet{shiny1}. Es kann bei kurzen Informationen im Internet aber auch reichen die Adresse\footnote{\url{https://shiny.rstudio.com/tutorial/written-tutorial/lesson1/}} als Fußnote einzubetten.

So werden andere Teile der Arbeit referenziert: Kapitel \ref{einleitung}, Gleichung \ref{eq:1} zeigt...

So verweisen wir auf eine Fußnote \footnote{dies ist eine Fußnote}.

\section{Abbildungen}

Abbildungen erfordern das package \textit{graphicx}. 
Idealerweise verwendet man Vektorgrafiken oder hochaufgelöste Bitmaps. 
Eine gute Variante ist das Verwenden von PDFs.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.3\textwidth]{siegel.pdf}
    \caption{Siegel der Universität}
    \label{fig:my_label}
\end{figure}


\section{Tabellen}

Die Tabular-Umgebung gibt die Anzahl Spalten an, deren Orientierung, Breite und evtl. Zwischenlinien. 


\begin{table}[ht]
    \centering
    \caption{Meine Tabelle}
        \begin{tabular}{ cccc } 
        \toprule
        col1 & col2 & col3 \\
        \midrule
        \multirow{3}{4em}{Multiple row} & cell2 & cell3 \\ 
        & cell5 & cell6 \\ 
        & cell8 & cell9 \\ 
        \bottomrule
    \end{tabular}
    \label{tab:countries}
\end{table}

\section{Formeln}

\begin{equation}
    \sum_{i=1}^N x_i
    \label{eq:1}
\end{equation}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Literaturverzeichnis wird 
%% automatisch eingefügt
%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\clearpage
\lhead{}
\printbibliography
\addcontentsline{toc}{section}{\bibname}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Anhang (optional) 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\clearpage
\appendix
\section{Anhang A}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Eidesstattliche Erklärung
%% muss angepasst werden 
%% in Erklaerung.tex
%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\input{Erklaerung.tex}

\end{document}
